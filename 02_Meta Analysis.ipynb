{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a0d939",
   "metadata": {},
   "source": [
    "# Read me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0832ff",
   "metadata": {},
   "source": [
    "## please run the Exploratory and Association Rules Analysis first. This code requires pickles from the Association Rule Mining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf4631f",
   "metadata": {},
   "source": [
    "METADATA\n",
    "\n",
    "trx_date: the date of the transaction;\n",
    "\n",
    "trx_id: the ID of each transaction;\n",
    "\n",
    "product_code: the ID of the product;\n",
    "\n",
    "sub_family: subfamily is a product hierarchy;\n",
    "\n",
    "family: family is a product hierarchy;\n",
    "\n",
    "quantity: number of product units;\n",
    "\n",
    "unit_price: the unit price of the product;\n",
    "\n",
    "flag_pastry: boolean flag to identify products that belong to the pastry category;\n",
    "\n",
    "shopID: the ID of the location/shop where the transaction occured;\n",
    "\n",
    "customerID: the ID of the customer in the purchase transaction;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e0971",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl\n",
    "!pip install mlxtend\n",
    "!pip install networkx matplotlib\n",
    "!pip install joypy\n",
    "!pip install factor_analyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a75708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_zscore(data, threshold=3):\n",
    "    z_scores = (data - data.mean()) / data.std()\n",
    "    filtered_data = data[abs(z_scores) <= threshold]\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "## defines a function that plots multiple box plots\n",
    "def plot_multiple_boxplots(data, feats, title=\"Box Plots\"):\n",
    "\n",
    "    # Prepare figure. Create individual axes where each histogram will be placed\n",
    "    fig, axes = plt.subplots(2, ceil(len(feats) / 2), figsize=(20, 11))\n",
    "\n",
    "    # Plot data\n",
    "    # Iterate across axes objects and associate each histogram (hint: use the ax.hist() instead of plt.hist()):\n",
    "    for ax, feat in zip(axes.flatten(), feats): # Notice the zip() function and flatten() method\n",
    "      sns.boxplot(x=data[feat], ax=ax)\n",
    "    \n",
    "    # Layout\n",
    "    # Add a centered title to the figure:\n",
    "    plt.suptitle(title)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def is_redundant(rule, rules_df):\n",
    "    \"\"\"\n",
    "    Check if a given rule is redundant within a set of rules.\n",
    "    A rule is redundant if there's another rule with the same consequent,\n",
    "    equal or higher confidence, and a subset of the antecedents.\n",
    "    \"\"\"\n",
    "    for _, other_rule in rules_df.iterrows():\n",
    "        if (rule['consequents'] == other_rule['consequents']) and \\\n",
    "           (rule['antecedents'].issubset(other_rule['antecedents'])) and \\\n",
    "           (rule['confidence'] <= other_rule['confidence']) and \\\n",
    "           (rule['antecedents'] != other_rule['antecedents']):  # Avoid comparing a rule to itself\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f20605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import ticker\n",
    "import joypy\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import KMeans\n",
    "from math import ceil\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import networkx as nx\n",
    "import matplotlib.colors as mcolors\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, leaves_list\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "from scipy.cluster.hierarchy import linkage as sch_linkage\n",
    "import pickle\n",
    "import warnings\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973c10da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9178fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data = pd.read_csv('anon_sales.csv')\n",
    "sales_data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a57ef6b",
   "metadata": {},
   "source": [
    "# Part I: Data Pre-Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694590f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary from the file\n",
    "with open('meta_dfs.pkl', 'rb') as file:\n",
    "    meta_dfs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary from the file\n",
    "with open('meta_dfs_2023.pkl', 'rb') as file:\n",
    "    meta_dfs_2023 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4021523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary from the file\n",
    "with open('meta_dfs_lift.pkl', 'rb') as file:\n",
    "    meta_dfs_lift = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04ce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_id_frozensets(df, columns=['antecedents', 'consequents']):\n",
    "    \"\"\"\n",
    "    Orders the ID frozensets within specified columns of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The DataFrame to process.\n",
    "    columns (list of str): List of column names to order frozensets in.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        # Convert frozenset to sorted list, then back to frozenset\n",
    "        df[col] = df[col].apply(lambda x: frozenset(sorted(x)))\n",
    "    return df\n",
    "\n",
    "# Apply the function to each DataFrame in meta_dfs\n",
    "for shop, years_dfs in meta_dfs.items():\n",
    "    for year, df in years_dfs.items():\n",
    "        meta_dfs[shop][year] = order_id_frozensets(df, columns=['antecedents', 'consequents'])\n",
    "\n",
    "# Now, meta_dfs has been updated with ordered frozensets in the specified columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ecf60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'sub_family_' with '' (empty string) to leave only the number part\n",
    "sales_data['family'] = sales_data['family'].str.replace('family_', '')\n",
    "\n",
    "# Optionally, if you want to convert the column to numeric type\n",
    "sales_data['family'] = pd.to_numeric(sales_data['family'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832cab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'sub_family_' with '' (empty string) to leave only the number part\n",
    "sales_data['sub_family'] = sales_data['sub_family'].str.replace('sub_family_', '')\n",
    "\n",
    "# Optionally, if you want to convert the column to numeric type\n",
    "sales_data['sub_family'] = pd.to_numeric(sales_data['sub_family'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73e56f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'sub_family_' with '' (empty string) to leave only the number part\n",
    "sales_data['shopID'] = sales_data['shopID'].str.replace('ShopID_', '')\n",
    "\n",
    "# Optionally, if you want to convert the column to numeric type\n",
    "sales_data['shopID'] = pd.to_numeric(sales_data['shopID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc291dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'sub_family_' with '' (empty string) to leave only the number part\n",
    "sales_data['product_code'] = sales_data['product_code'].str.replace('Product_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'trx_id' with '' (empty string) to leave only the number part\n",
    "sales_data['trx_id'] = sales_data['trx_id'].str.replace('trxID_', '')\n",
    "\n",
    "# Optionally, if you want to convert the column to numeric type\n",
    "sales_data['trx_id'] = pd.to_numeric(sales_data['trx_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e036b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'trx_date' column to datetime format\n",
    "sales_data['trx_date'] = pd.to_datetime(sales_data['trx_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d6ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data['unit_price'] = pd.to_numeric(sales_data['unit_price'].str.replace(',', '.'), errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa9cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new variable total_value as the product of quantity and unit_price\n",
    "sales_data['total_value'] = sales_data['quantity'] * sales_data['unit_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1336d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from original years to new years\n",
    "year_mapping = {\n",
    "    1992: 2020,\n",
    "    1993: 2021,\n",
    "    1994: 2022,\n",
    "    1995: 2023,\n",
    "    1996: 2024\n",
    "}\n",
    "\n",
    "# Apply the mapping to change the year, keeping month and day the same\n",
    "sales_data['trx_date'] = sales_data['trx_date'].apply(\n",
    "    lambda x: x.replace(year=year_mapping[x.year])\n",
    ")\n",
    "\n",
    "sales_data['trx_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ac62c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'year' column based on 'trx_date'\n",
    "sales_data['year'] = sales_data['trx_date'].dt.to_period('Y')\n",
    "\n",
    "# Create a 'quarter' column based on 'trx_date'\n",
    "sales_data['quarter'] = sales_data['trx_date'].dt.to_period('Q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf7aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'year_integer' column by extracting year and converting to integer\n",
    "sales_data['year_integer'] = sales_data['trx_date'].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2da7081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine season\n",
    "def get_season(month):\n",
    "    seasons = {'winter': (12, 1, 2), 'spring': (3, 4, 5), 'summer': (6, 7, 8), 'autumn': (9, 10, 11)}\n",
    "    for season, months in seasons.items():\n",
    "        if month in months:\n",
    "            return season\n",
    "    return None\n",
    "\n",
    "sales_data['season'] = sales_data['trx_date'].dt.month.apply(get_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bca023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day of the week (Monday=1, Tuesday=2, ...)\n",
    "sales_data['day_of_week_number'] = sales_data['trx_date'].dt.dayofweek + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0101f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day of the week name\n",
    "sales_data['day_of_week_name'] = sales_data['trx_date'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1f5cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data['year_month'] = sales_data['trx_date'].dt.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a49bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming meta_dfs is your dictionary containing all the shop dataframes\n",
    "all_data = []\n",
    "for shop, years in meta_dfs.items():\n",
    "    for year, df in years.items():\n",
    "        df['shop'] = shop\n",
    "        df['year'] = year\n",
    "        all_data.append(df)\n",
    "\n",
    "# Concatenate all the dataframes into one\n",
    "combined_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Create 'rule' column if it doesn't exist\n",
    "combined_data['rule'] = combined_data['LHS'] + ' → ' + combined_data['RHS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de01138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693ae1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the support values for each rule\n",
    "rule_supports = {}\n",
    "\n",
    "# Iterate over each shop and year to extract support values for each rule\n",
    "for shop, years_dfs in meta_dfs.items():\n",
    "    for year, df in years_dfs.items():\n",
    "        # Iterate through each row to extract rule support\n",
    "        for _, row in df.iterrows():\n",
    "            # Sort antecedents and consequents\n",
    "            sorted_antecedents = sorted(list(row['antecedents']), key=lambda x: int(x.split('_')[1]))\n",
    "            sorted_consequents = sorted(list(row['consequents']), key=lambda x: int(x.split('_')[1]))\n",
    "            \n",
    "            # Create a unique identifier for each rule\n",
    "            rule_identifier = f\"{tuple(sorted_antecedents)} -> {tuple(sorted_consequents)}\"\n",
    "            \n",
    "            # Initialize or update the support for the rule in the specific shop\n",
    "            rule_supports.setdefault(rule_identifier, {})[shop] = row['support']\n",
    "\n",
    "# Convert the rule supports into a DataFrame for easier manipulation\n",
    "supports_df = pd.DataFrame.from_dict(rule_supports, orient='index').fillna(0)\n",
    "supports_df.reset_index(inplace=True)\n",
    "supports_df.rename(columns={'index': 'Rule'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3812b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Initialize a Counter object to hold frequencies of antecedents and consequents\n",
    "item_frequencies = Counter()\n",
    "\n",
    "for shop, years_dfs in meta_dfs.items():\n",
    "    for year, df in years_dfs.items():\n",
    "        # Aggregate antecedents\n",
    "        for items in df['antecedents']:\n",
    "            item_frequencies.update(items)\n",
    "        # Aggregate consequents\n",
    "        for items in df['consequents']:\n",
    "            item_frequencies.update(items)\n",
    "\n",
    "# Convert the Counter object to a DataFrame for easier manipulation and visualization\n",
    "\n",
    "# Create a DataFrame from the Counter object\n",
    "freq_df = pd.DataFrame(item_frequencies.items(), columns=['Item', 'Frequency']).sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "# Ensure the 'Item' column is of type string for later operations\n",
    "freq_df['Item'] = freq_df['Item'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f36a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_frequencies = {}\n",
    "\n",
    "for shop, years_dfs in meta_dfs.items():\n",
    "    for year, df in years_dfs.items():\n",
    "        # Iterate through each row to construct sorted rule identifiers\n",
    "        for _, row in df.iterrows():\n",
    "            # Sort antecedents and consequents\n",
    "            sorted_antecedents = sorted(list(row['antecedents']), key=lambda x: int(x.split('_')[1]))\n",
    "            sorted_consequents = sorted(list(row['consequents']), key=lambda x: int(x.split('_')[1]))\n",
    "            \n",
    "            # Create a unique identifier for each rule\n",
    "            rule_identifier = f\"{tuple(sorted_antecedents)} -> {tuple(sorted_consequents)}\"\n",
    "            \n",
    "            if rule_identifier not in rule_frequencies:\n",
    "                rule_frequencies[rule_identifier] = {shop: 1}\n",
    "            else:\n",
    "                if shop in rule_frequencies[rule_identifier]:\n",
    "                    rule_frequencies[rule_identifier][shop] += 1\n",
    "                else:\n",
    "                    rule_frequencies[rule_identifier][shop] = 1\n",
    "\n",
    "# Convert the rule frequencies into a DataFrame for easier manipulation\n",
    "rules_df = pd.DataFrame.from_dict(rule_frequencies, orient='index').fillna(0)\n",
    "rules_df.reset_index(inplace=True)\n",
    "rules_df.rename(columns={'index': 'Rule'}, inplace=True)\n",
    "\n",
    "# Now, each rule string is sorted within itself by the numeric part of the ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4e578f",
   "metadata": {},
   "outputs": [],
   "source": [
    " rule_lifts = {}\n",
    "\n",
    "for shop, years_dfs in meta_dfs.items():\n",
    "    for year, df in years_dfs.items():\n",
    "        # Iterate through each row to extract rule lift\n",
    "        for _, row in df.iterrows():\n",
    "            # Sort antecedents and consequents\n",
    "            sorted_antecedents = sorted(list(row['antecedents']), key=lambda x: int(x.split('_')[1]))\n",
    "            sorted_consequents = sorted(list(row['consequents']), key=lambda x: int(x.split('_')[1]))\n",
    "            \n",
    "            # Create a unique identifier for each rule\n",
    "            rule_identifier = f\"{tuple(sorted_antecedents)} -> {tuple(sorted_consequents)}\"\n",
    "            \n",
    "            # Initialize or update the lift for the rule in the specific shop\n",
    "            rule_lifts.setdefault(rule_identifier, {})[shop] = row['lift']\n",
    "\n",
    "# Convert the rule lifts into a DataFrame for easier manipulation\n",
    "lifts_df = pd.DataFrame.from_dict(rule_lifts, orient='index').fillna(0)\n",
    "lifts_df.reset_index(inplace=True)\n",
    "lifts_df.rename(columns={'index': 'Rule'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cf221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the confidence values for each rule\n",
    "rule_confidences = {}\n",
    "\n",
    "# Iterate over each shop and year to extract confidence values for each rule\n",
    "for shop, years_dfs in meta_dfs.items():\n",
    "    for year, df in years_dfs.items():\n",
    "        # Iterate through each row to extract rule confidence\n",
    "        for _, row in df.iterrows():\n",
    "            # Sort antecedents and consequents\n",
    "            sorted_antecedents = sorted(list(row['antecedents']), key=lambda x: int(x.split('_')[1]))\n",
    "            sorted_consequents = sorted(list(row['consequents']), key=lambda x: int(x.split('_')[1]))\n",
    "            \n",
    "            # Create a unique identifier for each rule\n",
    "            rule_identifier = f\"{tuple(sorted_antecedents)} -> {tuple(sorted_consequents)}\"\n",
    "            \n",
    "            # Initialize or update the confidence for the rule in the specific shop\n",
    "            rule_confidences.setdefault(rule_identifier, {})[shop] = row['confidence']\n",
    "\n",
    "# Convert the rule confidences into a DataFrame for easier manipulation\n",
    "confidences_df = pd.DataFrame.from_dict(rule_confidences, orient='index').fillna(0)\n",
    "confidences_df.reset_index(inplace=True)\n",
    "confidences_df.rename(columns={'index': 'Rule'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b12bfab",
   "metadata": {},
   "source": [
    "# Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05054ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(94, 70))\n",
    "sns.heatmap(rules_df.set_index('Rule'), annot=True, fmt=\".0f\", cmap=\"YlGnBu\")\n",
    "plt.title('Rule Frequencies Across Shops')\n",
    "plt.ylabel('Rules')\n",
    "plt.xlabel('Shops')\n",
    "plt.xticks(fontsize=50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65524ae8",
   "metadata": {},
   "source": [
    "# Rule Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129143e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming supports_df is already in a suitable format for plotting\n",
    "plt.figure(figsize=(60, 55))\n",
    "sns.heatmap(supports_df.set_index('Rule'), annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
    "plt.title('Rule Support Across Shops')\n",
    "plt.ylabel('Rules')\n",
    "plt.xlabel('Shops')\n",
    "plt.xticks(fontsize=50)\n",
    "\n",
    "# Save the plot as an SVG file\n",
    "plt.savefig(\"rule_support_across_shops.svg\", format='svg')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ec87b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the total Support and count of occurrences for each rule\n",
    "total_support_per_rule = {}\n",
    "count_per_rule = {}\n",
    "\n",
    "# Iterate over each row to calculate the total Support and count of occurrences for each rule\n",
    "for _, row in supports_df.iterrows():\n",
    "    rule = row['Rule']\n",
    "    support_values = row.drop('Rule').values\n",
    "    for shop, support in zip(supports_df.columns[1:], support_values):\n",
    "        if support >= 0:  # Consider only non-negative support values\n",
    "            if rule not in total_support_per_rule:\n",
    "                total_support_per_rule[rule] = support\n",
    "                count_per_rule[rule] = 1\n",
    "            else:\n",
    "                total_support_per_rule[rule] += support\n",
    "                count_per_rule[rule] += 1\n",
    "\n",
    "# Calculate the average Support for each rule\n",
    "average_support_per_rule = {rule: total_support / count for rule, total_support, count in zip(total_support_per_rule.keys(), total_support_per_rule.values(), count_per_rule.values())}\n",
    "\n",
    "# Sort the rules based on their average Support values in descending order\n",
    "top_10_rules_support = sorted(average_support_per_rule.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Display the top 10 rules with the highest average Support\n",
    "print(\"Top 10 rules with the highest average Support:\")\n",
    "for rule, avg_support in top_10_rules_support:\n",
    "    total_support = total_support_per_rule.get(rule, 0)  # Get the total support or default to 0 if not present\n",
    "    count = count_per_rule.get(rule, 0)  # Get the count or default to 0 if not present\n",
    "    print(f\"Rule: {rule}, Average Support: {avg_support}, Total Support: {total_support}, Number of Occurrences: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9b9648",
   "metadata": {},
   "source": [
    "# Rule Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf3a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming confidences_df is already in a suitable format for plotting\n",
    "plt.figure(figsize=(60, 45))\n",
    "sns.heatmap(confidences_df.set_index('Rule'), annot=True, fmt=\".2f\", cmap=\"YlGnBu\")\n",
    "plt.title('Rule Confidence Across Shops')\n",
    "plt.ylabel('Rules')\n",
    "plt.xlabel('Shops')\n",
    "plt.xticks(fontsize=50)\n",
    "\n",
    "# Save the plot as an SVG file\n",
    "plt.savefig(\"rule_confidence_across_shops.svg\", format='svg')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ae659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the total Confidence and count of occurrences for each rule\n",
    "total_confidence_per_rule = {}\n",
    "count_per_rule = {}\n",
    "\n",
    "# Iterate over each row to calculate the total Confidence and count of occurrences for each rule\n",
    "for _, row in confidences_df.iterrows():\n",
    "    rule = row['Rule']\n",
    "    confidence_values = row.drop('Rule').values\n",
    "    for shop, confidence in zip(confidences_df.columns[1:], confidence_values):\n",
    "        if confidence >= 0:  # Consider only non-negative confidence values\n",
    "            if rule not in total_confidence_per_rule:\n",
    "                total_confidence_per_rule[rule] = confidence\n",
    "                count_per_rule[rule] = 1\n",
    "            else:\n",
    "                total_confidence_per_rule[rule] += confidence\n",
    "                count_per_rule[rule] += 1\n",
    "\n",
    "# Calculate the average Confidence for each rule\n",
    "average_confidence_per_rule = {rule: total_confidence / count for rule, total_confidence, count in zip(total_confidence_per_rule.keys(), total_confidence_per_rule.values(), count_per_rule.values())}\n",
    "\n",
    "# Sort the rules based on their average Confidence values in descending order\n",
    "top_10_rules_confidence = sorted(average_confidence_per_rule.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Display the top 10 rules with the highest average Confidence\n",
    "print(\"Top 10 rules with the highest average Confidence:\")\n",
    "for rule, avg_confidence in top_10_rules_confidence:\n",
    "    total_confidence = total_confidence_per_rule.get(rule, 0)  # Get the total confidence or default to 0 if not present\n",
    "    count = count_per_rule.get(rule, 0)  # Get the count or default to 0 if not present\n",
    "    print(f\"Rule: {rule}, Average Confidence: {avg_confidence}, Total Confidence: {total_confidence}, Number of Occurrences: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf28569",
   "metadata": {},
   "source": [
    "# Rule Lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a108d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(72, 62))\n",
    "sns.heatmap(lifts_df.set_index('Rule'), annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
    "plt.title('Rule Lift Across Shops')\n",
    "plt.ylabel('Rules')\n",
    "plt.xlabel('Shops')\n",
    "plt.xticks(fontsize=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45122ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f035974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the lifts_df DataFrame by the 'Lift' column in descending order\n",
    "lifts_df_new = lifts_df.drop(columns=['shop12'])\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(72, 62))\n",
    "sns.heatmap(lifts_df_new.set_index('Rule'), annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
    "plt.title('Rule Lift Across Shops (Excluding Top 3 Highest Lift)')\n",
    "plt.ylabel('Rules')\n",
    "plt.xlabel('Shops')\n",
    "plt.xticks(fontsize=50)\n",
    "\n",
    "# Save the plot as an SVG file\n",
    "svg_filename = \"rule_lift_across_shops.svg\"\n",
    "plt.savefig(svg_filename, format='svg')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b161f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the total Lift and count of occurrences for each rule\n",
    "total_lift_per_rule = {}\n",
    "count_per_rule = {}\n",
    "\n",
    "# Iterate over each row to calculate the total Lift and count of occurrences for each rule\n",
    "for _, row in lifts_df.iterrows():\n",
    "    rule = row['Rule']\n",
    "    lift_values = row.drop('Rule').values\n",
    "    for shop, lift in zip(lifts_df.columns[1:], lift_values):\n",
    "        if lift >= 0:  # Consider only non-negative lift values\n",
    "            if rule not in total_lift_per_rule:\n",
    "                total_lift_per_rule[rule] = lift\n",
    "                count_per_rule[rule] = 1\n",
    "            else:\n",
    "                total_lift_per_rule[rule] += lift\n",
    "                count_per_rule[rule] += 1\n",
    "\n",
    "# Calculate the average Lift for each rule\n",
    "average_lift_per_rule = {rule: total_lift / count for rule, total_lift, count in zip(total_lift_per_rule.keys(), total_lift_per_rule.values(), count_per_rule.values())}\n",
    "\n",
    "# Sort the rules based on their average Lift values in descending order\n",
    "top_10_rules_lift = sorted(average_lift_per_rule.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Display the top 10 rules with the highest average Lift\n",
    "print(\"Top 10 rules with the highest average Lift:\")\n",
    "for rule, avg_lift in top_10_rules_lift:\n",
    "    total_lift = total_lift_per_rule.get(rule, 0)  # Get the total lift or default to 0 if not present\n",
    "    count = count_per_rule.get(rule, 0)  # Get the count or default to 0 if not present\n",
    "    print(f\"Rule: {rule}, Average Lift: {avg_lift}, Total Lift: {total_lift}, Number of Occurrences: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a01974f",
   "metadata": {},
   "source": [
    "## Finding out the top rules based on confidence, lift and support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b48a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of converting a list to DataFrame if necessary\n",
    "if isinstance(top_10_rules_lift, list):\n",
    "    top_10_rules_lift = pd.DataFrame(top_10_rules_lift, columns=['Rule', 'Metric_Value'])\n",
    "if isinstance(top_10_rules_support, list):\n",
    "    top_10_rules_support = pd.DataFrame(top_10_rules_support, columns=['Rule', 'Metric_Value'])\n",
    "if isinstance(top_10_rules_confidence, list):\n",
    "    top_10_rules_confidence = pd.DataFrame(top_10_rules_confidence, columns=['Rule', 'Metric_Value'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4af945",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_rules_lift = top_10_rules_lift.sort_values(by='Metric_Value', ascending=False)\n",
    "top_10_rules_support = top_10_rules_support.sort_values(by='Metric_Value', ascending=False)\n",
    "top_10_rules_confidence = top_10_rules_confidence.sort_values(by='Metric_Value', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54081c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These DataFrames should have at least two columns: 'Rule' and 'Metric_Value' (where 'Metric_Value' could be lift, support, or confidence)\n",
    "\n",
    "# Corrected function to assign scores based on rank within the DataFrame with robust indexing\n",
    "def assign_scores(df):\n",
    "    num_rules = min(len(df), 10)  # To handle cases with fewer than 10 entries\n",
    "    # Resetting the index of DataFrame to ensure proper alignment when assigning scores\n",
    "    df = df.reset_index(drop=True)\n",
    "    # Create a new column 'Score' and initialize with 0\n",
    "    df['Score'] = 0\n",
    "    # Assign scores within the range of existing rows\n",
    "    for i in range(num_rules):\n",
    "        df.at[i, 'Score'] = 10 - i\n",
    "    return df[['Rule', 'Score']]\n",
    "\n",
    "\n",
    "# Apply the scoring function to each DataFrame\n",
    "scores_lift = assign_scores(top_10_rules_lift)\n",
    "scores_support = assign_scores(top_10_rules_support)\n",
    "scores_confidence = assign_scores(top_10_rules_confidence)\n",
    "\n",
    "# Combine all the scores into one DataFrame\n",
    "combined_scores = pd.merge(scores_lift, scores_support, on='Rule', how='outer', suffixes=('_lift', '_support'))\n",
    "combined_scores = pd.merge(combined_scores, scores_confidence, on='Rule', how='outer')\n",
    "combined_scores.fillna(0, inplace=True)  # Replace NaN scores (for rules not in top 10 in some metric) with 0\n",
    "\n",
    "# Calculate the total score for each rule\n",
    "combined_scores['Total_Score'] = combined_scores['Score_lift'] + combined_scores['Score_support'] + combined_scores['Score']\n",
    "\n",
    "# Sort by total score in descending order to find the highest ranked rules\n",
    "combined_scores.sort_values(by='Total_Score', ascending=False, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da794d9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_scores.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f31d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and file path\n",
    "directory = os.path.expanduser(\"~/Desktop/Master Project newest/data\")\n",
    "file_path = os.path.join(directory, \"combined_scores.csv\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "combined_scores.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"DataFrame saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3283d1e1",
   "metadata": {},
   "source": [
    "# Top 5 for 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed804a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_id_frozensets(df, columns=['antecedents', 'consequents']):\n",
    "    \"\"\"\n",
    "    Orders the ID frozensets within specified columns of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The DataFrame to process.\n",
    "    columns (list of str): List of column names to order frozensets in.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        # Convert frozenset to sorted list, then back to frozenset\n",
    "        df[col] = df[col].apply(lambda x: frozenset(sorted(x)))\n",
    "    return df\n",
    "\n",
    "# Apply the function to each DataFrame in meta_dfs\n",
    "for shop, years_dfs in meta_dfs_2023.items():\n",
    "    for year, df in years_dfs.items():\n",
    "        meta_dfs_2023[shop][year] = order_id_frozensets(df, columns=['antecedents', 'consequents'])\n",
    "\n",
    "# Now, meta_dfs has been updated with ordered frozensets in the specified columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4fb15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming meta_dfs is your dictionary containing all the shop dataframes\n",
    "all_data_2023 = []\n",
    "for shop, years in meta_dfs_2023.items():\n",
    "    for year, df in years.items():\n",
    "        df['shop'] = shop\n",
    "        df['year'] = year\n",
    "        all_data_2023.append(df)\n",
    "\n",
    "# Concatenate all the dataframes into one\n",
    "combined_data_2023 = pd.concat(all_data_2023, ignore_index=True)\n",
    "\n",
    "# Create 'rule' column if it doesn't exist\n",
    "combined_data_2023['rule'] = combined_data_2023['LHS'] + ' → ' + combined_data_2023['RHS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91fda09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the support values for each rule\n",
    "rule_supports_2023 = {}\n",
    "\n",
    "# Iterate over each shop and year to extract support values for each rule\n",
    "for shop_2023, years_dfs_2023 in meta_dfs.items():\n",
    "    for year_2023, df_2023 in years_dfs_2023.items():\n",
    "        # Iterate through each row to extract rule support\n",
    "        for _, row_2023 in df_2023.iterrows():\n",
    "            # Sort antecedents and consequents\n",
    "            sorted_antecedents_2023 = sorted(list(row_2023['antecedents']), key=lambda x: int(x.split('_')[1]))\n",
    "            sorted_consequents_2023 = sorted(list(row_2023['consequents']), key=lambda x: int(x.split('_')[1]))\n",
    "            \n",
    "            # Create a unique identifier for each rule\n",
    "            rule_identifier_2023 = f\"{tuple(sorted_antecedents_2023)} -> {tuple(sorted_consequents_2023)}\"\n",
    "            \n",
    "            # Initialize or update the support for the rule in the specific shop\n",
    "            rule_supports_2023.setdefault(rule_identifier_2023, {})[shop_2023] = row_2023['support']\n",
    "\n",
    "# Convert the rule supports into a DataFrame for easier manipulation\n",
    "supports_df_2023 = pd.DataFrame.from_dict(rule_supports_2023, orient='index').fillna(0)\n",
    "supports_df_2023.reset_index(inplace=True)\n",
    "supports_df_2023.rename(columns={'index': 'Rule'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75079703",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_lifts_2023 = {}\n",
    "\n",
    "for shop_2023, years_dfs_2023 in meta_dfs.items():\n",
    "    for year_2023, df_2023 in years_dfs_2023.items():\n",
    "        # Iterate through each row to extract rule lift\n",
    "        for _, row_2023 in df_2023.iterrows():\n",
    "            # Sort antecedents and consequents\n",
    "            sorted_antecedents_2023 = sorted(list(row_2023['antecedents']), key=lambda x: int(x.split('_')[1]))\n",
    "            sorted_consequents_2023 = sorted(list(row_2023['consequents']), key=lambda x: int(x.split('_')[1]))\n",
    "            \n",
    "            # Create a unique identifier for each rule\n",
    "            rule_identifier_2023 = f\"{tuple(sorted_antecedents_2023)} -> {tuple(sorted_consequents_2023)}\"\n",
    "            \n",
    "            # Initialize or update the lift for the rule in the specific shop\n",
    "            rule_lifts_2023.setdefault(rule_identifier_2023, {})[shop_2023] = row_2023['lift']\n",
    "\n",
    "# Convert the rule lifts into a DataFrame for easier manipulation\n",
    "lifts_df_2023 = pd.DataFrame.from_dict(rule_lifts_2023, orient='index').fillna(0)\n",
    "lifts_df_2023.reset_index(inplace=True)\n",
    "lifts_df_2023.rename(columns={'index': 'Rule'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f72fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the confidence values for each rule\n",
    "rule_confidences_2023 = {}\n",
    "\n",
    "# Iterate over each shop and year to extract confidence values for each rule\n",
    "for shop_2023, years_dfs_2023 in meta_dfs.items():\n",
    "    for year_2023, df_2023 in years_dfs_2023.items():\n",
    "        # Iterate through each row to extract rule confidence\n",
    "        for _, row_2023 in df_2023.iterrows():\n",
    "            # Sort antecedents and consequents\n",
    "            sorted_antecedents_2023 = sorted(list(row_2023['antecedents']), key=lambda x: int(x.split('_')[1]))\n",
    "            sorted_consequents_2023 = sorted(list(row_2023['consequents']), key=lambda x: int(x.split('_')[1]))\n",
    "            \n",
    "            # Create a unique identifier for each rule\n",
    "            rule_identifier_2023 = f\"{tuple(sorted_antecedents_2023)} -> {tuple(sorted_consequents_2023)}\"\n",
    "            \n",
    "            # Initialize or update the confidence for the rule in the specific shop\n",
    "            rule_confidences_2023.setdefault(rule_identifier_2023, {})[shop_2023] = row_2023['confidence']\n",
    "\n",
    "# Convert the rule confidences into a DataFrame for easier manipulation\n",
    "confidences_df_2023 = pd.DataFrame.from_dict(rule_confidences_2023, orient='index').fillna(0)\n",
    "confidences_df_2023.reset_index(inplace=True)\n",
    "confidences_df_2023.rename(columns={'index': 'Rule'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d6dc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the total Support and count of occurrences for each rule\n",
    "total_support_per_rule_2023 = {}\n",
    "count_per_rule_2023 = {}\n",
    "\n",
    "# Iterate over each row to calculate the total Support and count of occurrences for each rule\n",
    "for _, row_2023 in supports_df_2023.iterrows():\n",
    "    rule_2023 = row_2023['Rule']\n",
    "    support_values_2023 = row_2023.drop('Rule').values\n",
    "    for shop_2023, support_2023 in zip(supports_df_2023.columns[1:], support_values_2023):\n",
    "        if support_2023 >= 0:  # Consider only non-negative support values\n",
    "            if rule_2023 not in total_support_per_rule_2023:\n",
    "                total_support_per_rule_2023[rule_2023] = support_2023\n",
    "                count_per_rule_2023[rule_2023] = 1\n",
    "            else:\n",
    "                total_support_per_rule_2023[rule_2023] += support_2023\n",
    "                count_per_rule_2023[rule_2023] += 1\n",
    "\n",
    "# Calculate the average Support for each rule\n",
    "average_support_per_rule_2023 = {rule_2023: total_support_2023 / count_2023 for rule_2023, total_support_2023, count_2023 in zip(total_support_per_rule_2023.keys(), total_support_per_rule_2023.values(), count_per_rule_2023.values())}\n",
    "\n",
    "# Sort the rules based on their average Support values in descending order\n",
    "top_10_rules_support_2023 = sorted(average_support_per_rule_2023.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Display the top 10 rules with the highest average Support\n",
    "print(\"Top 10 rules with the highest average Support:\")\n",
    "for rule_2023, avg_support_2023 in top_10_rules_support_2023:\n",
    "    total_support_2023 = total_support_per_rule_2023.get(rule_2023, 0)  # Get the total support or default to 0 if not present\n",
    "    count_2023 = count_per_rule_2023.get(rule_2023, 0)  # Get the count or default to 0 if not present\n",
    "    print(f\"Rule: {rule_2023}, Average Support: {avg_support_2023}, Total Support: {total_support_2023}, Number of Occurrences: {count_2023}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556ef4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the total Confidence and count of occurrences for each rule\n",
    "total_confidence_per_rule_2023 = {}\n",
    "count_per_rule_2023 = {}\n",
    "\n",
    "# Iterate over each row to calculate the total Confidence and count of occurrences for each rule\n",
    "for _, row_2023 in confidences_df_2023.iterrows():\n",
    "    rule_2023 = row_2023['Rule']\n",
    "    confidence_values_2023 = row_2023.drop('Rule').values\n",
    "    for shop_2023, confidence_2023 in zip(confidences_df_2023.columns[1:], confidence_values_2023):\n",
    "        if confidence_2023 >= 0:  # Consider only non-negative confidence values\n",
    "            if rule_2023 not in total_confidence_per_rule_2023:\n",
    "                total_confidence_per_rule_2023[rule_2023] = confidence_2023\n",
    "                count_per_rule_2023[rule_2023] = 1\n",
    "            else:\n",
    "                total_confidence_per_rule_2023[rule_2023] += confidence_2023\n",
    "                count_per_rule_2023[rule_2023] += 1\n",
    "\n",
    "# Calculate the average Confidence for each rule\n",
    "average_confidence_per_rule_2023 = {rule_2023: total_confidence_2023 / count_2023 for rule_2023, total_confidence_2023, count_2023 in zip(total_confidence_per_rule_2023.keys(), total_confidence_per_rule_2023.values(), count_per_rule_2023.values())}\n",
    "\n",
    "# Sort the rules based on their average Confidence values in descending order\n",
    "top_10_rules_confidence_2023 = sorted(average_confidence_per_rule_2023.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Display the top 10 rules with the highest average Confidence\n",
    "print(\"Top 10 rules with the highest average Confidence:\")\n",
    "for rule_2023, avg_confidence_2023 in top_10_rules_confidence_2023:\n",
    "    total_confidence_2023 = total_confidence_per_rule_2023.get(rule_2023, 0)  # Get the total confidence or default to 0 if not present\n",
    "    count_2023 = count_per_rule_2023.get(rule_2023, 0)  # Get the count or default to 0 if not present\n",
    "    print(f\"Rule: {rule_2023}, Average Confidence: {avg_confidence_2023}, Total Confidence: {total_confidence_2023}, Number of Occurrences: {count_2023}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4fccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the total Lift and count of occurrences for each rule\n",
    "total_lift_per_rule_2023 = {}\n",
    "count_per_rule_2023 = {}\n",
    "\n",
    "# Iterate over each row to calculate the total Lift and count of occurrences for each rule\n",
    "for _, row_2023 in lifts_df_2023.iterrows():\n",
    "    rule_2023 = row_2023['Rule']\n",
    "    lift_values_2023 = row_2023.drop('Rule').values\n",
    "    for shop_2023, lift_2023 in zip(lifts_df_2023.columns[1:], lift_values_2023):\n",
    "        if lift_2023 >= 0:  # Consider only non-negative lift values\n",
    "            if rule_2023 not in total_lift_per_rule_2023:\n",
    "                total_lift_per_rule_2023[rule_2023] = lift_2023\n",
    "                count_per_rule_2023[rule_2023] = 1\n",
    "            else:\n",
    "                total_lift_per_rule_2023[rule_2023] += lift_2023\n",
    "                count_per_rule_2023[rule_2023] += 1\n",
    "\n",
    "# Calculate the average Lift for each rule\n",
    "average_lift_per_rule_2023 = {rule_2023: total_lift_2023 / count_2023 for rule_2023, total_lift_2023, count_2023 in zip(total_lift_per_rule_2023.keys(), total_lift_per_rule_2023.values(), count_per_rule_2023.values())}\n",
    "\n",
    "# Sort the rules based on their average Lift values in descending order\n",
    "top_10_rules_lift_2023 = sorted(average_lift_per_rule_2023.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Display the top 10 rules with the highest average Lift\n",
    "print(\"Top 10 rules with the highest average Lift:\")\n",
    "for rule_2023, avg_lift_2023 in top_10_rules_lift_2023:\n",
    "    total_lift_2023 = total_lift_per_rule_2023.get(rule_2023, 0)  # Get the total lift or default to 0 if not present\n",
    "    count_2023 = count_per_rule_2023.get(rule_2023, 0)  # Get the count or default to 0 if not present\n",
    "    print(f\"Rule: {rule_2023}, Average Lift: {avg_lift_2023}, Total Lift: {total_lift_2023}, Number of Occurrences: {count_2023}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcd0165",
   "metadata": {},
   "source": [
    "## Finding out the top rules based on confidence, lift and support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of converting a list to DataFrame if necessary\n",
    "if isinstance(top_10_rules_lift_2023, list):\n",
    "    top_10_rules_lift_2023 = pd.DataFrame(top_10_rules_lift_2023, columns=['Rule_2023', 'Metric_Value_2023'])\n",
    "if isinstance(top_10_rules_support_2023, list):\n",
    "    top_10_rules_support_2023 = pd.DataFrame(top_10_rules_support_2023, columns=['Rule_2023', 'Metric_Value_2023'])\n",
    "if isinstance(top_10_rules_confidence_2023, list):\n",
    "    top_10_rules_confidence_2023 = pd.DataFrame(top_10_rules_confidence_2023, columns=['Rule_2023', 'Metric_Value_2023'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff9cf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_rules_lift_2023 = top_10_rules_lift_2023.sort_values(by='Metric_Value_2023', ascending=False)\n",
    "top_10_rules_support_2023 = top_10_rules_support_2023.sort_values(by='Metric_Value_2023', ascending=False)\n",
    "top_10_rules_confidence_2023 = top_10_rules_confidence_2023.sort_values(by='Metric_Value_2023', ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1119ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These DataFrames should have at least two columns: 'Rule' and 'Metric_Value' (where 'Metric_Value' could be lift, support, or confidence)\n",
    "\n",
    "# Corrected function to assign scores based on rank within the DataFrame with robust indexing\n",
    "def assign_scores_2023(df_2023):\n",
    "    num_rules_2023 = min(len(df_2023), 10)  # To handle cases with fewer than 10 entries\n",
    "    # Resetting the index of DataFrame to ensure proper alignment when assigning scores\n",
    "    df_2023 = df_2023.reset_index(drop=True)\n",
    "    # Create a new column 'Score' and initialize with 0\n",
    "    df_2023['Score_2023'] = 0\n",
    "    # Assign scores within the range of existing rows\n",
    "    for i in range(num_rules_2023):\n",
    "        df_2023.at[i, 'Score_2023'] = 10 - i\n",
    "    return df_2023[['Rule_2023', 'Score_2023']]\n",
    "\n",
    "# Note: The function now ensures the index is reset and uses the .at method for precise assignment.\n",
    "# The other code sections for merging and totaling scores should be reused with this updated function definition.\n",
    "# Uncomment when ready to implement in your local environment.\n",
    "\n",
    "# Apply the scoring function to each DataFrame\n",
    "scores_lift_2023 = assign_scores_2023(top_10_rules_lift_2023)\n",
    "scores_support_2023 = assign_scores_2023(top_10_rules_support_2023)\n",
    "scores_confidence_2023 = assign_scores_2023(top_10_rules_confidence_2023)\n",
    "\n",
    "# Combine all the scores into one DataFrame\n",
    "combined_scores_2023 = pd.merge(scores_lift_2023, scores_support_2023, on='Rule_2023', how='outer', suffixes=('_lift_2023', '_support_2023'))\n",
    "combined_scores_2023 = pd.merge(combined_scores_2023, scores_confidence_2023, on='Rule_2023', how='outer')\n",
    "combined_scores_2023.fillna(0, inplace=True)  # Replace NaN scores (for rules not in top 10 in some metric) with 0\n",
    "\n",
    "# Correct the calculation of the total score\n",
    "combined_scores_2023['Total_Score_2023'] = combined_scores_2023['Score_2023_lift_2023'] + combined_scores_2023['Score_2023_support_2023'] + combined_scores_2023['Score_2023']\n",
    "\n",
    "\n",
    "# Sort by total score in descending order to find the highest ranked rules\n",
    "combined_scores_2023.sort_values(by='Total_Score_2023', ascending=False, inplace=True)\n",
    "\n",
    "# Uncomment to view the DataFrame with the top rules based on their combined scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5b4db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_scores_2023.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dde2beb",
   "metadata": {},
   "source": [
    "# Meta Analysis, Time Series Shop 1 Search 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dad1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by 'rule', 'shop', and 'year'\n",
    "rule_metrics_time = combined_data.groupby(['rule', 'shop', 'year']).agg({\n",
    "    'support': 'mean',\n",
    "    'confidence': 'mean',\n",
    "    'lift': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate the maximum year for each shop and convert it into a DataFrame for merging\n",
    "max_year_per_shop = rule_metrics_time.groupby('shop')['year'].max().reset_index()\n",
    "max_year_per_shop.rename(columns={'year': 'year_max'}, inplace=True)\n",
    "\n",
    "# Merge the maximum year information back into the rule metrics\n",
    "rule_first_last_year = rule_metrics_time.groupby(['rule', 'shop']).agg(\n",
    "    first_year=('year', 'min'),\n",
    "    last_year=('year', 'max')\n",
    ").reset_index()\n",
    "\n",
    "rule_first_last_year = rule_first_last_year.merge(max_year_per_shop, on='shop', how='left')\n",
    "rule_first_last_year['is_current'] = rule_first_last_year['last_year'] == rule_first_last_year['year_max']\n",
    "\n",
    "# Filter to analyze further or visualize trends\n",
    "print(rule_first_last_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd81375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out active rules\n",
    "active_rules = rule_first_last_year[rule_first_last_year['is_current']]\n",
    "\n",
    "# Printing active rules for verification\n",
    "print(active_rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f968c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge active rules with original metrics to get the full data for these rules\n",
    "active_rule_metrics = active_rules.merge(rule_metrics_time, on=['rule', 'shop'], how='left')\n",
    "\n",
    "# Aggregate to get the mean confidence for each rule across all years it was active\n",
    "active_rule_confidence = active_rule_metrics.groupby(['rule', 'shop']).agg({\n",
    "    'confidence': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Aggregate to get the mean confidence for each rule across all years it was active\n",
    "active_rule_support = active_rule_metrics.groupby(['rule', 'shop']).agg({\n",
    "    'support': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "# Aggregate to get the mean confidence for each rule across all years it was active\n",
    "active_rule_lift = active_rule_metrics.groupby(['rule', 'shop']).agg({\n",
    "    'lift': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2195132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the results by confidence for better visualization\n",
    "active_rule_confidence_sorted = active_rule_confidence.sort_values(by='confidence', ascending=False)\n",
    "\n",
    "# Sorting the results by Lift for better visualization\n",
    "active_rule_lift_sorted = active_rule_lift.sort_values(by='lift', ascending=False)\n",
    "\n",
    "# Sorting the results by Support for better visualization\n",
    "active_rule_support_sorted = active_rule_support.sort_values(by='support', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_rule_lift_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacc87c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames based on the 'Rule' column\n",
    "combined_df = pd.merge(active_rule_lift_sorted, active_rule_support_sorted, on='rule')\n",
    "combined_df = pd.merge(combined_df, active_rule_confidence_sorted, on='rule')\n",
    "\n",
    "# Calculate a combined score based on lift, support, and confidence\n",
    "# You can define your own formula for combining the scores based on your business requirements\n",
    "# For example, you can simply sum the three metrics or assign different weights to each metric\n",
    "combined_df['Combined_Score'] = combined_df['lift'] + combined_df['support'] + combined_df['confidence']\n",
    "\n",
    "# Sort the DataFrame by the combined score in descending order\n",
    "combined_df = combined_df.sort_values(by='Combined_Score', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01935f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting up the plot\n",
    "plt.figure(figsize=(10, 30))\n",
    "sns.barplot(x='confidence', y='rule', data=active_rule_confidence_sorted, hue='shop', dodge=False)\n",
    "plt.title('Mean Confidence of Active Rules by Shop')\n",
    "plt.xlabel('Mean Confidence')\n",
    "plt.ylabel('Rule')\n",
    "plt.legend(title='Shop')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d50da33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting up the plot\n",
    "plt.figure(figsize=(10, 30))\n",
    "sns.barplot(x='lift', y='rule', data=active_rule_lift_sorted, hue='shop', dodge=False)\n",
    "plt.title('Mean Lift of Active Rules by Shop')\n",
    "plt.xlabel('Mean Lift')\n",
    "plt.ylabel('Rule')\n",
    "plt.legend(title='Shop')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb7b186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting up the plot\n",
    "plt.figure(figsize=(10, 30))\n",
    "sns.barplot(x='support', y='rule', data=active_rule_support_sorted, hue='shop', dodge=False)\n",
    "plt.title('Mean Support of Active Rules by Shop')\n",
    "plt.xlabel('Mean Support')\n",
    "plt.ylabel('Rule')\n",
    "plt.legend(title='Shop')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb560bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ad98ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Normalize the metrics (support, confidence, and lift) to be between 0 and 1\n",
    "combined_df['normalized_support'] = (combined_df['support'] - combined_df['support'].min()) / (combined_df['support'].max() - combined_df['support'].min())\n",
    "combined_df['normalized_confidence'] = (combined_df['confidence'] - combined_df['confidence'].min()) / (combined_df['confidence'].max() - combined_df['confidence'].min())\n",
    "combined_df['normalized_lift'] = (combined_df['lift'] - combined_df['lift'].min()) / (combined_df['lift'].max() - combined_df['lift'].min())\n",
    "\n",
    "# Calculate the combined score for each rule using normalized metrics\n",
    "combined_df['combined_score_normalized'] = combined_df['normalized_support'] + combined_df['normalized_confidence'] + combined_df['normalized_lift']\n",
    "\n",
    "# Sorting the results by the combined score for better visualization\n",
    "combined_df_sorted_normalized = combined_df.sort_values(by='combined_score_normalized', ascending=False)\n",
    "\n",
    "# Setting up the plot\n",
    "plt.figure(figsize=(10, 30))\n",
    "sns.barplot(x='combined_score_normalized', y='rule', data=combined_df_sorted_normalized, hue='shop', dodge=False)\n",
    "plt.title('Combined Score of Active Rules by Shop (Normalized)')\n",
    "plt.xlabel('Combined Score (Normalized)')\n",
    "plt.ylabel('Rule')\n",
    "plt.legend(title='Shop')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484a8e16",
   "metadata": {},
   "source": [
    "# Ideas\n",
    "\n",
    "From here we can observe, which active rule has the highest value in the different metrics. This leads to different managerial implications:\n",
    "\n",
    "### Strategic Implications of Association Rule Metrics\n",
    "\n",
    "**1. Lift:**\n",
    "Lift is a metric that measures how much more often the antecedent and consequent of a rule occur together than we would expect if they were statistically independent. A rule with a high lift value indicates a strong, positive association between the antecedent and consequent. **Strategic implications:** Rules with high lift are particularly valuable for cross-selling strategies because they identify items that strongly influence the purchase of other items. For instance, a high lift between bread and butter suggests that customers who buy bread are much more likely to buy butter than the average shopper. Promotions can be structured to capitalize on these strong associations, potentially by bundling items together or positioning them close to each other in store layouts or online categories.\n",
    "\n",
    "**2. Support:**\n",
    "Support measures the proportion of transactions that include both the antecedent and the consequent. It gives an idea of how frequently a rule is applicable in the dataset. **Strategic implications:** A rule with high support impacts a larger segment of the transactions, making it crucial for mass-market strategies. High-support rules are robust bases for making general stocking and marketing decisions, as they affect a significant portion of customer transactions. For example, if milk and cereal have high support, this rule can guide store inventory decisions to ensure these popular items are always in stock, optimizing turnover rates.\n",
    "\n",
    "**3. Confidence:**\n",
    "Confidence assesses the reliability of the inference made by the rule, calculated as the probability of seeing the consequent given the antecedent. **Strategic implications:** High confidence in a rule indicates that the presence of the antecedent significantly increases the likelihood of the consequent’s presence in transactions. This metric is critical for targeting marketing campaigns and personalizing recommendations. For instance, if there is high confidence that customers who purchase running shoes also buy sports drinks, targeted offers can be made to these customers, increasing the probability of additional purchases.\n",
    "\n",
    "### How Strategic Implications Differ by Metric:\n",
    "\n",
    "- **Lift vs. Support vs. Confidence:** While support indicates the usefulness and productivity of a rule across all transactions, lift provides insight into the strength of an association independent of the prevalence of the outcome, making it ideal for uncovering hidden relationships in data. Confidence, on the other hand, helps in understanding the likelihood or reliability of predicting the consequent, thus being directly actionable for personalized marketing strategies.\n",
    "- **Decision Making:** High lift values can identify niche but potentially profitable associations that might be overlooked when only considering support. In contrast, high confidence rules can be misleading if based on rare item combinations, unless verified with substantial support or lift. Therefore, combining these metrics provides a balanced view: lift to find valuable associations, support to assess broad impact, and confidence to ensure predictability in targeted actions.\n",
    "- **Resource Allocation:** Understanding these metrics helps in prioritizing resource allocation in promotional activities, inventory management, and personalized marketing. For example, high lift and confidence rules might direct more aggressive cross-promotion tactics, whereas high support might influence broader stock level decisions.\n",
    "\n",
    "By strategically analyzing these metrics, businesses can tailor their operational and marketing strategies more effectively, ensuring that they not only meet the general demand but also exploit specific, strong item associations to enhance customer satisfaction and increase sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3646c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have the initial data setup and grouped correctly, like this:\n",
    "# rule_metrics_time = combined_data.groupby(['rule', 'shop', 'year']).agg({\n",
    "#     'support': 'mean',\n",
    "#     'confidence': 'mean',\n",
    "#     'lift': 'mean'\n",
    "# }).reset_index()\n",
    "\n",
    "# Prepare the growth_decay DataFrame\n",
    "growth_decay = rule_first_last_year.copy()\n",
    "\n",
    "# Calculate growth and decay rates for support, confidence, and lift\n",
    "growth_decay['support_change'] = growth_decay.apply(\n",
    "    lambda x: (\n",
    "        rule_metrics_time[(rule_metrics_time['rule'] == x['rule']) & (rule_metrics_time['shop'] == x['shop']) & (rule_metrics_time['year'] == x['last_year'])]['support'].values[0] -\n",
    "        rule_metrics_time[(rule_metrics_time['rule'] == x['rule']) & (rule_metrics_time['shop'] == x['shop']) & (rule_metrics_time['year'] == x['first_year'])]['support'].values[0]\n",
    "    ) / rule_metrics_time[(rule_metrics_time['rule'] == x['rule']) & (rule_metrics_time['shop'] == x['shop']) & (rule_metrics_time['year'] == x['first_year'])]['support'].values[0] if x['first_year'] != x['last_year'] else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "growth_decay['confidence_change'] = growth_decay.apply(\n",
    "    lambda x: (\n",
    "        rule_metrics_time[(rule_metrics_time['rule'] == x['rule']) & (rule_metrics_time['shop'] == x['shop']) & (rule_metrics_time['year'] == x['last_year'])]['confidence'].values[0] -\n",
    "        rule_metrics_time[(rule_metrics_time['rule'] == x['rule']) & (rule_metrics_time['shop'] == x['shop']) & (rule_metrics_time['year'] == x['first_year'])]['confidence'].values[0]\n",
    "    ) / rule_metrics_time[(rule_metrics_time['rule'] == x['rule']) & (rule_metrics_time['shop'] == x['shop']) & (rule_metrics_time['year'] == x['first_year'])]['confidence'].values[0] if x['first_year'] != x['last_year'] else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "growth_decay['lift_change'] = growth_decay.apply(\n",
    "    lambda x: (\n",
    "        rule_metrics_time[(rule_metrics_time['rule'] == x['rule']) & (rule_metrics_time['shop'] == x['shop']) & (rule_metrics_time['year'] == x['last_year'])]['lift'].values[0] -\n",
    "        rule_metrics_time[(rule_metrics_time['rule'] == x['rule']) & (rule_metrics_time['shop'] == x['shop']) & (rule_metrics_time['year'] == x['first_year'])]['lift'].values[0]\n",
    "    ) / rule_metrics_time[(rule_metrics_time['rule'] == x['rule']) & (rule_metrics_time['shop'] == x['shop']) & (rule_metrics_time['year'] == x['first_year'])]['lift'].values[0] if x['first_year'] != x['last_year'] else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Now, filter for significant changes\n",
    "significant_confidence_changes = growth_decay[growth_decay['confidence_change'].abs() > 0.1]\n",
    "significant_lift_changes = growth_decay[growth_decay['lift_change'].abs() > 0.1]\n",
    "significant_support_changes = growth_decay[growth_decay['support_change'].abs() > 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f16f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by 'confidence_change' in descending order to show the most significant changes first\n",
    "sorted_significant_changes = significant_confidence_changes.sort_values(by='confidence_change', ascending=False)\n",
    "\n",
    "# Now, create the bar plot with the sorted data\n",
    "plt.figure(figsize=(10, 6))\n",
    "bar_plot = sns.barplot(x='confidence_change', y='rule', data=sorted_significant_changes, hue='shop', dodge=False)\n",
    "plt.title('Significant Confidence Changes in Rules')\n",
    "plt.xlabel('Confidence Change Rate')\n",
    "plt.ylabel('Rule')\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot as an SVG file\n",
    "plt.savefig(\"significant_confidence_changes.svg\", format='svg', bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31124fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by 'lift_change' in descending order to show the most significant changes first\n",
    "sorted_significant_changes = significant_lift_changes.sort_values(by='lift_change', ascending=False)\n",
    "\n",
    "# Create the bar plot with the sorted data\n",
    "plt.figure(figsize=(12, 8))  # Increase figure size for better visibility\n",
    "bar_plot = sns.barplot(x='lift_change', y='rule', data=sorted_significant_changes, hue='shop', dodge=False)\n",
    "plt.title('Significant Lift Changes in Rules')\n",
    "plt.xlabel('Lift Change Rate')\n",
    "plt.ylabel('Rule')\n",
    "plt.grid(True)\n",
    "\n",
    "# Adjust layout to make sure everything fits\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as an SVG file\n",
    "plt.savefig(\"significant_lift_changes.svg\", format='svg', bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b4643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the bar plot with sorted data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sorted_significant_changes = significant_support_changes.sort_values(by='support_change', ascending=False)\n",
    "sns.barplot(x='support_change', y='rule', data=sorted_significant_changes, hue='shop', dodge=False)\n",
    "plt.title('Significant Support Changes in Rules (Sorted)')\n",
    "plt.xlabel('Support Change Rate')\n",
    "plt.ylabel('Rule')\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot as an SVG file\n",
    "plt.savefig(\"significant_support_changes.svg\", format='svg', bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a247089d",
   "metadata": {},
   "source": [
    "# Meta Analysis, Multi dimensional Scaling, Search 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming combined_data is already defined and has 'shop', 'antecedents', and 'consequents' columns\n",
    "\n",
    "# Create a custom rule representation using tuples of sorted antecedents and consequents\n",
    "def create_rule(antecedents, consequents):\n",
    "    return (tuple(sorted(antecedents)), tuple(sorted(consequents)))\n",
    "\n",
    "# Create a dictionary to accumulate rules for each shop\n",
    "shops = combined_data['shop'].unique()\n",
    "shop_rules = {shop: set() for shop in shops}\n",
    "\n",
    "for _, row in combined_data.iterrows():\n",
    "    rule = create_rule(row['antecedents'], row['consequents'])\n",
    "    shop_rules[row['shop']].add(rule)\n",
    "\n",
    "# Check rules for Shop11 in shop_rules\n",
    "print(f\"Rules for Shop11 in shop_rules:\\n{shop_rules['shop11']}\")\n",
    "print(f\"Number of unique rules for Shop11 in shop_rules: {len(shop_rules['shop11'])}\")\n",
    "\n",
    "# Create a binary matrix where each row represents a shop and each column a unique rule\n",
    "all_rules = list(set.union(*shop_rules.values()))  # Convert the set to a list\n",
    "rule_matrix = pd.DataFrame(0, index=shops, columns=all_rules, dtype=int)\n",
    "\n",
    "for shop in shops:\n",
    "    for rule in shop_rules[shop]:\n",
    "        rule_matrix.at[shop, rule] = 1\n",
    "\n",
    "# Convert DataFrame to numpy array for distance calculation\n",
    "rule_matrix_np = rule_matrix.to_numpy()\n",
    "\n",
    "# Compute the similarity matrix using Jaccard similarity\n",
    "similarity_matrix = 1 - pairwise_distances(rule_matrix_np, metric='jaccard')\n",
    "\n",
    "# Apply MDS\n",
    "mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "shop_positions = mds.fit_transform(similarity_matrix)  # Already 1 - similarity_matrix gives dissimilarity\n",
    "\n",
    "# Prepare results for plotting\n",
    "shop_positions_df = pd.DataFrame(shop_positions, index=shops, columns=['x', 'y'])\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(15, 10))\n",
    "for shop in shops:\n",
    "    plt.scatter(shop_positions_df.at[shop, 'x'], shop_positions_df.at[shop, 'y'], label=shop)\n",
    "    plt.text(shop_positions_df.at[shop, 'x'] + 0.001, shop_positions_df.at[shop, 'y'] + 0.001, shop)  # Slight offset for label\n",
    "\n",
    "plt.title('2D MDS of Shop Similarities in Association Rules')\n",
    "plt.xlabel('MDS Dimension 1')\n",
    "plt.ylabel('MDS Dimension 2')\n",
    "plt.legend(title='Shops')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb898e",
   "metadata": {},
   "source": [
    "How can it be, that shop 7,8,9 have similarities in the heatmap, are in the same cluster, but look so differently in the MDS? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7ee9a9",
   "metadata": {},
   "source": [
    "# Meta Analysis, Clustering to see differences, Search 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4c2b1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Suppress specific DeprecationWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, message=\"is_sparse is deprecated\")\n",
    "\n",
    "\n",
    "# Example DataFrame structure\n",
    "data = {\n",
    "    'shop': combined_data['shop'],\n",
    "    'rule': combined_data['antecedents'].astype(str) + '->' + combined_data['consequents'].astype(str),\n",
    "    'confidence': combined_data['confidence']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Pivot table to transform data into a [shops x rules] matrix, filling missing rules with zero\n",
    "feature_matrix = df.pivot_table(index='shop', columns='rule', values='confidence', fill_value=0)\n",
    "\n",
    "# Normalize the feature matrix\n",
    "scaler = MinMaxScaler()\n",
    "feature_matrix_normalized = scaler.fit_transform(feature_matrix)\n",
    "feature_matrix_normalized = pd.DataFrame(feature_matrix_normalized, index=feature_matrix.index, columns=feature_matrix.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12af798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1182f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b98973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal number of clusters using the elbow method\n",
    "sse = {}\n",
    "for k in range(1, 9):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(feature_matrix_normalized)\n",
    "    sse[k] = kmeans.inertia_  # Sum of squared distances of samples to their closest cluster center\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(list(sse.keys()), list(sse.values()))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.title(\"Elbow Method For Optimal k\")\n",
    "plt.show()\n",
    "\n",
    "# Assume the elbow is found at k=4\n",
    "optimal_k = 4\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "clusters = kmeans.fit_predict(feature_matrix_normalized)\n",
    "\n",
    "# Add cluster information back to the original DataFrame\n",
    "feature_matrix['Cluster'] = clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04061385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the rest of the preprocessing and clustering code has been run, starting from PCA\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(feature_matrix_normalized)\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "pca_df['Cluster'] = clusters\n",
    "\n",
    "# Assuming 'pca_df' already has PCA results and cluster labels\n",
    "# Let's add shop identifiers to the DataFrame for easy access\n",
    "pca_df['Shop'] = feature_matrix.index  # This assumes the index of your feature matrix has shop identifiers\n",
    "\n",
    "# Now plotting with annotations\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot = sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=pca_df, palette='viridis', s=100, alpha=0.7)\n",
    "\n",
    "# Adding text annotations for each point\n",
    "for i in range(pca_df.shape[0]):\n",
    "    plt.text(x=pca_df.PC1[i] + 0.02,  # x-coordinate position for text\n",
    "             y=pca_df.PC2[i] + 0.02,  # y-coordinate position for text\n",
    "             s=pca_df.Shop[i],  # text label\n",
    "             fontdict=dict(color='black', size=10),\n",
    "            )\n",
    "\n",
    "plt.title('PCA Cluster Plot of Shops with Shop Labels')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot as SVG\n",
    "svg_filename = \"pca_cluster_plot.svg\"\n",
    "plt.savefig(svg_filename, format='svg')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plot saved as {svg_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8aeb72",
   "metadata": {},
   "source": [
    "1. Data Attributes\n",
    "The primary attributes used for clustering were based on association rules, specifically:\n",
    "\n",
    "Rules: Each unique combination of antecedents (items that lead to a purchase) and consequents (items that are purchased as a result) formed a rule.\n",
    "Confidence: The strength of each rule, which measures the reliability of the rule, was used as the key attribute. Confidence is defined as the probability of seeing the consequent items given the antecedent items have been purchased.\n",
    "\n",
    "2. Construction of the Feature Matrix\n",
    "The feature matrix was constructed as follows:\n",
    "\n",
    "Rows: Each row represented a shop.\n",
    "\n",
    "Columns: Each column represented a unique association rule derived from the combined data of all shops. The uniqueness of a rule was based on the specific combination of items in the antecedents and consequents.\n",
    "\n",
    "Values: The value in each cell of the matrix was the confidence level of the rule for that shop. If a shop did not have a particular rule present in its transactions, the confidence was set to zero.\n",
    "\n",
    "3. Data Preparation for Clustering\n",
    "Normalization: The feature matrix was normalized to ensure that each rule was equally weighted during the clustering process. Normalization adjusted the confidence values so that they were scaled between 0 and 1 across the dataset. This step is crucial because it prevents rules with naturally higher confidence levels from disproportionately influencing the clustering.\n",
    "\n",
    "4. Clustering Algorithm\n",
    "K-Means Clustering: This algorithm was chosen for its efficiency and effectiveness in grouping data into clusters that minimize the variance within each cluster. The number of clusters (k) was determined using the elbow method, which identifies a point where adding more clusters does not significantly improve the within-cluster sum of squares (SSE).\n",
    "\n",
    "5. Dimensionality Reduction for Visualization\n",
    "PCA (Principal Component Analysis): Though not directly involved in forming the clusters, PCA was used post-clustering to reduce the high-dimensional feature space into 2 dimensions. This reduction allowed for easy visualization of the clusters and helped interpret how shops are grouped based on the rules' presence and strength.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "The clusters were formed based on how similarly shops behaved concerning the association rules prevalent in their transactions. By using confidence as the clustering attribute, the analysis not only considered which rules were common but also how significant these rules were in each shop's context. This approach provided a nuanced view that combined both the qualitative aspect (which rules are present) and the quantitative aspect (how strong these rules are), leading to a comprehensive grouping based on purchasing patterns. This method is particularly useful for understanding market dynamics, tailoring marketing strategies, and optimizing inventory management based on consumer behavior similarities across shops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30765f1",
   "metadata": {},
   "source": [
    "# Meta Analysis, Hierarchical Clustering, MDS to see differences, Search 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a8cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'rule_matrix' is already prepared and contains binary rule presence data\n",
    "\n",
    "# Normalize rule_matrix for hierarchical clustering\n",
    "scaler = MinMaxScaler()\n",
    "rule_matrix_scaled = scaler.fit_transform(rule_matrix)\n",
    "\n",
    "# Hierarchical Clustering\n",
    "hclust = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')\n",
    "hclusters = hclust.fit_predict(rule_matrix_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d32e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the linkage matrix\n",
    "Z = sch_linkage(rule_matrix_scaled, method='ward')\n",
    "\n",
    "# Plotting the dendrogram\n",
    "plt.figure(figsize=(15, 10))\n",
    "dendrogram(Z, labels=shops, leaf_rotation=90, leaf_font_size=12, color_threshold=0)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Shop')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4cfd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MDS based on the Jaccard dissimilarity\n",
    "mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "shop_positions = mds.fit_transform(1 - similarity_matrix)  # using 1 - similarity_matrix for dissimilarity\n",
    "\n",
    "# Prepare results for plotting\n",
    "shop_positions_df = pd.DataFrame(shop_positions, columns=['x', 'y'], index=shops)\n",
    "shop_positions_df['MDS Cluster'] = hclusters  # add hierarchical clusters for coloring in MDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf9a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the MDS results\n",
    "plt.figure(figsize=(15, 10))\n",
    "scatter = plt.scatter(shop_positions_df['x'], shop_positions_df['y'], c=shop_positions_df['MDS Cluster'], cmap='viridis')\n",
    "plt.title('2D MDS of Shop Similarities in Association Rules with Hierarchical Clustering')\n",
    "plt.xlabel('MDS Dimension 1')\n",
    "plt.ylabel('MDS Dimension 2')\n",
    "plt.colorbar(scatter, label='Cluster Group')\n",
    "for idx, row in shop_positions_df.iterrows():\n",
    "    plt.text(row['x'], row['y'], s=idx, fontdict=dict(color='red', size=12))\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2617041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Defining a list of shop identifiers\n",
    "sorted_shops = ['shop1', 'shop11', 'shop12', 'shop13','shop3', 'shop4', 'shop5', 'shop6', 'shop7', 'shop8', 'shop9']\n",
    "print(\"Original order of sorted_shops:\")\n",
    "print(sorted_shops)\n",
    "\n",
    "# Function to extract numbers from shop identifiers\n",
    "def extract_number(text):\n",
    "    num = re.search(r'\\d+', text)\n",
    "    return int(num.group()) if num else None\n",
    "\n",
    "# Sort the list by extracting numbers and sorting based on them\n",
    "sorted_shops = sorted(sorted_shops, key=extract_number)\n",
    "\n",
    "print(\"\\nNumerically sorted sorted_shops:\")\n",
    "print(sorted_shops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da86e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddd6e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert hclusters to a pandas Series with shops as the index\n",
    "hclusters_series = pd.Series(hclusters, index=sorted_shops)\n",
    "\n",
    "# Reindex hclusters_series to match pca_df\n",
    "# This ensures that cluster labels are aligned with the PCA data\n",
    "hclusters_aligned = hclusters_series.reindex(pca_df['Shop']).values\n",
    "\n",
    "# Assign the correctly ordered clusters to pca_df\n",
    "pca_df['Cluster'] = hclusters_aligned\n",
    "\n",
    "# Now plotting with annotations\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot = sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=pca_df, palette='viridis', s=100, alpha=0.7)\n",
    "\n",
    "# Adding text annotations for each point\n",
    "for i in range(pca_df.shape[0]):\n",
    "    plt.text(x=pca_df.PC1[i] + 0.02,  # x-coordinate position for text\n",
    "             y=pca_df.PC2[i] + 0.02,  # y-coordinate position for text\n",
    "             s=pca_df.Shop[i],  # text label\n",
    "             fontdict=dict(color='black', size=10),\n",
    "            )\n",
    "\n",
    "plt.title('PCA Hierarchical Cluster Plot of Shops with Shop Labels')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network graph including all possible edges, with edge width varying by similarity\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add all nodes for each shop\n",
    "for shop in shops:\n",
    "    G.add_node(shop)\n",
    "\n",
    "# Connect all shops with edges weighted by similarity\n",
    "for i, shop_i in enumerate(shops):\n",
    "    for j, shop_j in enumerate(shops):\n",
    "        if i < j:  # Avoid self-loops and duplicate edges\n",
    "            similarity = similarity_matrix[i, j]\n",
    "            # Use a continuous scale for edge width\n",
    "            G.add_edge(shop_i, shop_j, weight=similarity)\n",
    "\n",
    "# Calculate the number of rules per shop for node sizes\n",
    "num_rules_per_shop = {shop: len(rules) for shop, rules in shop_rules.items()}\n",
    "\n",
    "# Normalize the values to use for node size\n",
    "sizes = [num_rules_per_shop[shop] for shop in G.nodes()]\n",
    "max_size = max(sizes)\n",
    "min_size = min(sizes)\n",
    "node_sizes = [(size - min_size) / (max_size - min_size) * 1000 + 100 for size in sizes]  # Scale between 100 and 1100\n",
    "\n",
    "# Use the 'MDS Cluster' for coloring nodes\n",
    "cluster_colors = shop_positions_df['MDS Cluster']\n",
    "\n",
    "# If the cluster numbers are not sequential starting from 0, map them to a sequential range\n",
    "unique_clusters = sorted(cluster_colors.unique())\n",
    "cluster_color_mapping = {cluster: i for i, cluster in enumerate(unique_clusters)}\n",
    "sequential_cluster_colors = cluster_colors.map(cluster_color_mapping)\n",
    "\n",
    "# Assuming G and all related variables are already defined as you have done.\n",
    "\n",
    "# Generate a color palette with seaborn and map the clusters to colors using viridis\n",
    "palette = sns.color_palette('viridis', len(unique_clusters))\n",
    "node_colors = [palette[color] for color in sequential_cluster_colors]\n",
    "\n",
    "# Draw the network with the updated color palette\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(G, seed=42)  # Positions should be consistent with previous plot\n",
    "\n",
    "# Draw nodes with sizes based on the number of rules and color based on clusters using the viridis palette\n",
    "nodes = nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, alpha=0.7)\n",
    "\n",
    "# Draw edges\n",
    "edges = nx.draw_networkx_edges(G, pos, width=[G[u][v]['weight']*5 for u, v in G.edges()], alpha=0.5)\n",
    "\n",
    "# Draw labels\n",
    "labels = nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "\n",
    "plt.title('Network of Shops Based on Rule Similarities (Colored by Hierarchical Clusters)')\n",
    "plt.axis('off')  # Turn off the axis if not needed\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df2b0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'feature_matrix_normalized' and 'shops' are already defined\n",
    "# K-means clustering has been performed with the determined optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)  # Change the number of clusters based on your elbow method results\n",
    "clusters = kmeans.fit_predict(feature_matrix_normalized)\n",
    "\n",
    "# Add cluster labels back to the original DataFrame for use in plotting\n",
    "feature_matrix['Cluster'] = clusters  # Assumes feature_matrix is a DataFrame with shops as rows\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add all nodes for each shop, including cluster info for coloring\n",
    "for shop, data in feature_matrix.iterrows():\n",
    "    G.add_node(shop, cluster=data['Cluster'])\n",
    "\n",
    "# Connect all shops with edges weighted by similarity\n",
    "for i, shop_i in enumerate(shops):\n",
    "    for j, shop_j in enumerate(shops):\n",
    "        if i < j:  # Avoid self-loops and duplicate edges\n",
    "            similarity = similarity_matrix[i, j]\n",
    "            G.add_edge(shop_i, shop_j, weight=similarity)\n",
    "\n",
    "# Calculate node sizes based on the number of unique rules compared to the other shops in their cluster\n",
    "unique_rules_per_shop = {}\n",
    "for shop in shops:\n",
    "    cluster = feature_matrix.at[shop, 'Cluster']\n",
    "    shops_in_cluster = feature_matrix[feature_matrix['Cluster'] == cluster].index\n",
    "    all_rules_in_cluster = set()\n",
    "    for shop_in_cluster in shops_in_cluster:\n",
    "        all_rules_in_cluster.update(shop_rules[shop_in_cluster])\n",
    "    unique_rules_per_shop[shop] = len((all_rules_in_cluster) - set(shop_rules[shop])) / len(all_rules_in_cluster)\n",
    "\n",
    "# Ignore shop 4 for min size calculation\n",
    "sizes = [unique_rules_per_shop.get(shop, 0) for shop in G.nodes() if shop != 'shop4']\n",
    "max_size = max(sizes)\n",
    "min_size = min(sizes)\n",
    "node_sizes = [(unique_rules_per_shop.get(shop, 0) - min_size) / (max_size - min_size) * 2500 + 100 for shop in G.nodes()]\n",
    "\n",
    "# Use cluster information for coloring nodes\n",
    "cluster_colors = [G.nodes[shop]['cluster'] for shop in G.nodes()]\n",
    "\n",
    "# Draw the network\n",
    "plt.figure(figsize=(10, 10))\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=cluster_colors, cmap='viridis', alpha=0.7)\n",
    "nx.draw_networkx_edges(G, pos, width=[G[u][v]['weight']*5 for u, v in G.edges()], alpha=0.5)\n",
    "nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "plt.title('Network of Shops Based on Rule Similarities (Colored by K-means Clusters)')\n",
    "plt.axis('off')\n",
    "\n",
    "# Save the plot as an SVG file\n",
    "plt.savefig(\"network_of_shops.svg\", format='svg')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot saved as network_of_shops.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491a37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate node sizes based on the number of unique rules compared to the other shops in their cluster\n",
    "unique_rules_per_shop = {}\n",
    "\n",
    "for shop in shops:\n",
    "    cluster = feature_matrix.at[shop, 'Cluster']\n",
    "    shops_in_cluster = feature_matrix[feature_matrix['Cluster'] == cluster].index\n",
    "    all_rules_in_cluster = set()\n",
    "    \n",
    "    for shop_in_cluster in shops_in_cluster:\n",
    "        all_rules_in_cluster.update(shop_rules[shop_in_cluster])\n",
    "    \n",
    "    unique_rules_per_shop[shop] = len((all_rules_in_cluster) - set(shop_rules[shop])) / len(all_rules_in_cluster)\n",
    "\n",
    "# Print the unique rules proportion for each shop\n",
    "for shop, value in unique_rules_per_shop.items():\n",
    "    print(f\"Shop: {shop}\\nUnique Rules Proportion: {value:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2be31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate node sizes based on the number of unique rules compared to the other shops in their cluster\n",
    "unique_rules_per_shop = {}\n",
    "excluded_shop = 'shop7'  # Shop to be excluded\n",
    "\n",
    "for shop in shops:\n",
    "    if shop == excluded_shop:\n",
    "        continue\n",
    "    \n",
    "    cluster = feature_matrix.at[shop, 'Cluster']\n",
    "    shops_in_cluster = feature_matrix[feature_matrix['Cluster'] == cluster].index\n",
    "    all_rules_in_cluster = set()\n",
    "    \n",
    "    for shop_in_cluster in shops_in_cluster:\n",
    "        if shop_in_cluster != excluded_shop:\n",
    "            all_rules_in_cluster.update(shop_rules[shop_in_cluster])\n",
    "    \n",
    "    unique_rules_per_shop[shop] = len((all_rules_in_cluster) - set(shop_rules[shop])) / len(all_rules_in_cluster)\n",
    "\n",
    "# Print the unique rules proportion for each shop\n",
    "for shop, value in unique_rules_per_shop.items():\n",
    "    print(f\"Shop: {shop}\\nUnique Rules Proportion: {value:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128c0fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len((shop_rules[shop])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e1a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the keys in shop_rules to ensure the shop IDs are correct\n",
    "print(\"Keys in shop_rules:\", shop_rules.keys())\n",
    "\n",
    "# Define shop IDs to analyze\n",
    "shop_ids = ['shop11', 'shop12', 'shop13']  # Use string keys if needed\n",
    "\n",
    "# Find the number of rules each shop has\n",
    "num_rules_per_shop = {shop: len(shop_rules[shop]) for shop in shop_ids}\n",
    "\n",
    "# Find the rules shared between each pair of shops\n",
    "shared_rules_11_12 = shop_rules['shop11'].intersection(shop_rules['shop12'])\n",
    "shared_rules_11_13 = shop_rules['shop11'].intersection(shop_rules['shop13'])\n",
    "shared_rules_12_13 = shop_rules['shop12'].intersection(shop_rules['shop13'])\n",
    "\n",
    "# Find the rules shared among all three shops\n",
    "shared_rules_all = shop_rules['shop11'].intersection(shop_rules['shop12'], shop_rules['shop13'])\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nNumber of rules per shop:\")\n",
    "for shop, num_rules in num_rules_per_shop.items():\n",
    "    print(f\"Shop {shop}: {num_rules} rules\")\n",
    "\n",
    "print(\"\\nShared rules between shop pairs:\")\n",
    "print(f\"Shared rules between shop 11 and shop 12: {len(shared_rules_11_12)}\")\n",
    "print(f\"Shared rules between shop 11 and shop 13: {len(shared_rules_11_13)}\")\n",
    "print(f\"Shared rules between shop 12 and shop 13: {len(shared_rules_12_13)}\")\n",
    "\n",
    "print(\"\\nShared rules among all three shops:\")\n",
    "print(f\"Shared rules among shop 11, shop 12, and shop 13: {len(shared_rules_all)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d9eed1",
   "metadata": {},
   "source": [
    "\n",
    "### Identifying Influential Shops\n",
    "- **Central Nodes**: Shops located at the center of the network might be influential in terms of customer behavior or product trends. They could be used as models for other shops or as test sites for new products or marketing strategies.\n",
    "- **Peripheral Nodes**: Shops on the periphery, with fewer connections, might have unique customer demographics or preferences. These could be targeted for more customized approaches.\n",
    "\n",
    "### Understanding Shop Relationships\n",
    "- **Cluster Analysis**: Shops colored similarly and clustered together indicate similar customer purchasing patterns. Managers can develop standardized strategies across these shops, such as shared inventory or coordinated marketing campaigns.\n",
    "- **Cross-Promotion Opportunities**: If two shops are connected but belong to different clusters, there might be an opportunity for cross-promotion or sharing of best practices to align their strategies better.\n",
    "\n",
    "### Resource Allocation\n",
    "- **Node Size as an Indicator**: If node size represents the number of unique rules, larger nodes might indicate shops with a broader variety of customer transactions, which could be prioritized for a diverse stock inventory.\n",
    "- **Investment Prioritization**: Larger and more central shops might be prioritized for investments, renovations, or technology upgrades due to their potential impact on the network.\n",
    "\n",
    "### Collaborations and Knowledge Sharing\n",
    "- **Sharing of Best Practices**: Shops with strong connections can be places where knowledge sharing is facilitated, helping to spread best practices across the network.\n",
    "- **Benchmarking Performance**: Managers can use network structure to set benchmarks or performance goals for shops based on the behavior of their 'neighbors' in the network.\n",
    "\n",
    "### Strategic Planning\n",
    "- **Market Positioning**: The network can help in understanding market positioning of each shop, suggesting which shops could be flag bearers for brand image or service quality.\n",
    "- **Expansion or Restructuring**: For expansion plans or restructuring, shops that occupy central roles or that bridge clusters might be critical to maintain.\n",
    "\n",
    "### Challenges and Risks\n",
    "- **Risk Mitigation**: Central shops might be more susceptible to systematic risks (e.g., supply chain disruptions). Understanding their role can help in mitigating these risks.\n",
    "- **Competition Analysis**: If there’s data on competitors, seeing how your shops cluster in relation to competitors' shops could indicate areas of competitive strength or weakness.\n",
    "\n",
    "### Personalized Strategies\n",
    "- **Customized Marketing**: For peripheral shops or those with unique connections, more personalized marketing strategies could be developed to target the specific customer base effectively.\n",
    "\n",
    "This network graph is a snapshot of the relational dynamics based on rule similarities. It is a strategic tool that can guide various aspects of business planning and operations. Each observation from the graph should be considered in the broader context of business goals, market conditions, and other qualitative data to make informed decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f08c3c7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Reducing Risk\n",
    "1. **Predictive Analytics for Inventory Management**:\n",
    "   - **Utilize Confidence**: High confidence rules can predict product demand more accurately. For example, if you find high confidence in the rule {winter coats} → {gloves}, you can stock gloves proportionally to winter coats to meet expected demand without overstocking.\n",
    "   - **Analyze Seasonal Variations**: Use temporal variations in rule strength to adjust inventory levels seasonally, minimizing the risk of overstocking perishable or seasonal items.\n",
    "\n",
    "2. **Dynamic Pricing Strategies**:\n",
    "   - **Employ Lift Analysis**: Products with high lift values can be bundled together at a slight discount to encourage sales while maintaining a higher combined profit margin than selling the items separately at a deeper discount.\n",
    "\n",
    "### Generating More Revenue\n",
    "1. **Cross-Selling and Up-Selling**:\n",
    "   - **Capitalize on Lift and Support**: Identify strong item associations with high lift and support to drive promotional strategies. For instance, place items that are frequently bought together in close proximity, both in-store and on online platforms, to boost impulse purchases.\n",
    "   - **Tailored Marketing Campaigns**: Use confidence metrics to target customers with personalized marketing, suggesting products they are likely to buy based on their shopping history.\n",
    "\n",
    "2. **Enhanced Customer Experience**:\n",
    "   - **Recommendation Systems**: Implement sophisticated recommendation engines that use high-confidence rules to personalize suggestions in real time, enhancing the shopping experience and increasing sales.\n",
    "\n",
    "### Reduce Costs\n",
    "1. **Optimized Inventory Management**:\n",
    "   - **Risk Mitigation via Rule Analysis**: Reduce carrying costs by using predictive analytics from rule confidence to optimize stock levels, ensuring that inventory levels match predicted sales without resulting in dead stock.\n",
    "   - **Reduce Wastage**: For perishable goods, applying lift and support metrics can help predict optimal stock levels, reducing spoilage and waste.\n",
    "\n",
    "2. **Efficient Resource Allocation**:\n",
    "   - **Marketing and Promotions**: Focus marketing efforts and promotional budgets on high-lift, high-support item combinations that are statistically proven to drive sales, ensuring marketing spend yields high returns.\n",
    "   - **Store Layout Optimization**: Reorganize store layouts based on item association rules to streamline customer flows and improve the shopping experience, potentially reducing labor costs associated with frequently rearranged displays.\n",
    "\n",
    "### Connecting the Dots: Business Perspective\n",
    "- **Data-Driven Decisions**: Your analysis empowers businesses to make informed decisions that are backed by data, reducing guesswork and the potential for costly mistakes.\n",
    "- **Strategic Implementation**: By understanding not only what items are purchased together but also the strength and reliability of these purchases, businesses can strategically target their efforts in areas with the greatest potential return on investment.\n",
    "- **Balanced Approach**: While generating revenue and reducing costs are direct benefits, reducing risk through predictive insights helps stabilize operations and ensures long-term sustainability.\n",
    "\n",
    "By incorporating these strategies, businesses can leverage your analytical findings to not only survive in competitive markets but thrive by proactively responding to consumer behavior patterns revealed through your research. This holistic approach to applying association rule mining encapsulates a comprehensive business strategy that optimizes performance across multiple fronts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ba00a6",
   "metadata": {},
   "source": [
    "To leverage your association rule analysis effectively toward achieving the goals of reducing risk, generating revenue, and preventing loss, here are concise strategies:\n",
    "\n",
    "### 1. **Reduce Risk**\n",
    "\n",
    "**Strategy**: Focus on the stability of high confidence rules across different times.\n",
    "- **Implementation**: Regularly track changes in the confidence levels of your association rules, especially during different seasons or days of the week. Identify rules that maintain high confidence regardless of external factors. Prioritize stability over peak values to mitigate risk associated with volatile market conditions or consumer behaviors.\n",
    "\n",
    "### 2. **Generate Revenue**\n",
    "\n",
    "**Strategy**: Capitalize on high lift values to promote products strategically.\n",
    "- **Implementation**: Use insights from days or seasons with unusually high lift scores to create targeted promotions or bundle offers. For example, if certain product combinations show significantly higher lift during specific seasons, schedule marketing campaigns to promote these bundles during those times to maximize cross-selling opportunities.\n",
    "\n",
    "### 3. **Prevent Loss**\n",
    "\n",
    "**Strategy**: Monitor transaction trends to avoid stock-outs and overstock.\n",
    "- **Implementation**: Analyze transaction count trends across different days of the week and seasons to forecast demand more accurately. Use this data to optimize your inventory levels—increasing stock before predicted high-transaction periods and reducing it when a decrease is anticipated. This approach helps in maintaining a balance, thus preventing both overstock and stock-outs, which are direct contributors to loss.\n",
    "\n",
    "By systematically applying these strategies, you can create a proactive business model that not only reacts to current trends but also anticipates future changes, thereby securing your business against potential risks, optimizing revenue opportunities, and minimizing losses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a02b90",
   "metadata": {},
   "source": [
    "# Low Sales, high Confidence & Lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ad1243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a rule string\n",
    "def create_rule_string(row):\n",
    "    return f\"{row['antecedents']} -> {row['consequents']}\"\n",
    "\n",
    "# Flatten the meta_dfs into a single DataFrame\n",
    "all_rules_data = []\n",
    "for shop, years_data in meta_dfs.items():\n",
    "    for year, df in years_data.items():\n",
    "        df = df.copy()  # Avoid modifying the original DataFrame\n",
    "        df['shop'] = shop\n",
    "        df['year'] = year\n",
    "        df['rule'] = df.apply(create_rule_string, axis=1)\n",
    "        df['product_codes'] = df['antecedents'].apply(list) + df['consequents'].apply(list)\n",
    "        all_rules_data.append(df[['rule', 'shop', 'year', 'lift', 'confidence', 'support', 'product_codes']])\n",
    "\n",
    "combined_rules_df = pd.concat(all_rules_data, ignore_index=True)\n",
    "\n",
    "# Explode the DataFrame so each product code is on its own row\n",
    "exploded_rules_df = combined_rules_df.explode('product_codes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c059ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming sales_data is already loaded as shown in your example\n",
    "# Merge the exploded rules DataFrame with sales_data on product_code, including trx_date and support\n",
    "merged_df = exploded_rules_df.merge(sales_data[['product_code', 'total_value', 'trx_date']], left_on='product_codes', right_on='product_code', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42066ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the total sales value for each rule and collect list of transaction dates and support\n",
    "final_df = merged_df.groupby(['rule', 'shop', 'year', 'lift', 'confidence']).agg(\n",
    "    total_sales_value=('total_value', 'sum'),\n",
    "    transaction_dates=('trx_date', list),  # Collect all transaction dates for each rule\n",
    "    support=('support', 'mean')  # Ensure to capture the support value appropriately\n",
    ").reset_index()\n",
    "\n",
    "# Display the DataFrame with rules and their corresponding transaction dates and support\n",
    "print(final_df[['rule', 'shop', 'year', 'lift', 'confidence', 'support', 'total_sales_value', 'transaction_dates']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8e099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae3c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a threshold for 'low sales' as the 25th percentile of the total sales values\n",
    "low_sales_threshold = final_df['total_sales_value'].quantile(0.1)\n",
    "\n",
    "# Filter for rules with high confidence (> 0.6) and high lift (> 5) but low sales value\n",
    "high_performance_rules = final_df[\n",
    "    (final_df['confidence'] > 0.6) & \n",
    "    (final_df['lift'] > 5) & \n",
    "    (final_df['total_sales_value'] <= low_sales_threshold)\n",
    "]\n",
    "\n",
    "print(high_performance_rules[['rule', 'shop', 'year', 'lift', 'confidence', 'total_sales_value']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c824d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_performance_rules.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2102d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the high_performance_rules DataFrame by 'total_sales_value' in ascending order\n",
    "sorted_high_performance_rules = high_performance_rules.sort_values(by='total_sales_value', ascending=True)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "print(sorted_high_performance_rules[['rule', 'shop', 'year', 'lift', 'confidence', 'total_sales_value']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563bf32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options to expand the width of the display for a DataFrame column\n",
    "pd.set_option('display.max_colwidth', None)  # None means no truncation\n",
    "\n",
    "# Now, when you print the DataFrame, it should show the full content of each 'rule'\n",
    "print(sorted_high_performance_rules[['rule']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9302ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_rule(rule):\n",
    "    try:\n",
    "        antecedents, consequents = rule.split(' -> ')  # ensure the split string matches the actual data\n",
    "        antecedents = ', '.join(sorted(eval(antecedents)))\n",
    "        consequents = ', '.join(sorted(eval(consequents)))\n",
    "        return f\"{antecedents} → {consequents}\"\n",
    "    except ValueError as e:\n",
    "        print(f\"Error processing rule: {rule}\")\n",
    "        return None  # or you can return rule to see which rule is causing the problem\n",
    "\n",
    "# Assuming 'sorted_high_performance_rules' is your DataFrame\n",
    "# Apply this function to the 'rule' column and store the result in a new column\n",
    "sorted_high_performance_rules['formatted_rule'] = sorted_high_performance_rules['rule'].apply(format_rule)\n",
    "\n",
    "# Print each formatted rule\n",
    "print(sorted_high_performance_rules[['rule', 'formatted_rule']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ec3cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted_high_performance_rules.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a503ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_high_performance_rules = sorted_high_performance_rules[['rule', 'shop', 'lift', 'confidence', 'total_sales_value', 'support']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890cb94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_high_performance_rules.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fd90ab",
   "metadata": {},
   "source": [
    "# Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964e2595",
   "metadata": {},
   "source": [
    "Although we can see some Rules that ahve low Sales and high Sales, this is not transferable to all the shops. I would recommend to go in each shop individually. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603dce7f",
   "metadata": {},
   "source": [
    "# Meta Analysis, Search 16 Filtered Based on Lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0389176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_id_frozensets(df, columns=['antecedents', 'consequents']):\n",
    "    \"\"\"\n",
    "    Orders the ID frozensets within specified columns of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The DataFrame to process.\n",
    "    columns (list of str): List of column names to order frozensets in.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        # Convert frozenset to sorted list, then back to frozenset\n",
    "        df[col] = df[col].apply(lambda x: frozenset(sorted(x)))\n",
    "    return df\n",
    "\n",
    "# Apply the function to each DataFrame in meta_dfs\n",
    "for shop, years_dfs_lift in meta_dfs_lift.items():\n",
    "    for year, df in years_dfs_lift.items():\n",
    "        meta_dfs_lift[shop][year] = order_id_frozensets(df, columns=['antecedents', 'consequents'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69117b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming meta_dfs is your dictionary containing all the shop dataframes for lift\n",
    "all_data_lift = []  # This will hold all the dataframes\n",
    "for shop, years in meta_dfs_lift.items():  # Iterating over the meta_dfs dictionary\n",
    "    for year, df in years.items():  # Iterating over each year dataframe within a shop\n",
    "        df['shop'] = shop  # Assigning the shop name to a new column\n",
    "        df['year'] = year  # Assigning the year to a new column\n",
    "        all_data_lift.append(df)  # Appending the dataframe to the list\n",
    "\n",
    "# Concatenate all the dataframes into one\n",
    "combined_data_lift = pd.concat(all_data_lift, ignore_index=True)\n",
    "\n",
    "# Create 'rule' column by concatenating 'LHS' and 'RHS'\n",
    "combined_data_lift['rule'] = combined_data_lift['LHS'].astype(str) + ' → ' + combined_data_lift['RHS'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb5256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Counter object to hold frequencies of antecedents and consequents\n",
    "item_frequencies_lift = Counter()\n",
    "\n",
    "for shop, years_dfs_lift in meta_dfs_lift.items():\n",
    "    for year, df in years_dfs_lift.items():\n",
    "        # Aggregate antecedents\n",
    "        for items in df['antecedents']:\n",
    "            item_frequencies_lift.update(items)\n",
    "        # Aggregate consequents\n",
    "        for items in df['consequents']:\n",
    "            item_frequencies_lift.update(items)\n",
    "\n",
    "# Convert the Counter object to a DataFrame for easier manipulation and visualization\n",
    "\n",
    "# Create a DataFrame from the Counter object\n",
    "freq_df_lift = pd.DataFrame(item_frequencies_lift.items(), columns=['Item', 'Frequency']).sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "# Ensure the 'Item' column is of type string for later operations\n",
    "freq_df_lift['Item'] = freq_df_lift['Item'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c53e56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_frequencies_lift = {}\n",
    "\n",
    "for shop, years_dfs_lift in meta_dfs_lift.items():\n",
    "    for year, df in years_dfs_lift.items():\n",
    "        # Iterate through each row to construct sorted rule identifiers\n",
    "        for _, row in df.iterrows():\n",
    "            # Sort antecedents and consequents\n",
    "            sorted_antecedents = sorted(list(row['antecedents']), key=lambda x: int(x.split('_')[1]))\n",
    "            sorted_consequents = sorted(list(row['consequents']), key=lambda x: int(x.split('_')[1]))\n",
    "            \n",
    "            # Create a unique identifier for each rule\n",
    "            rule_identifier_lift = f\"{tuple(sorted_antecedents)} -> {tuple(sorted_consequents)}\"\n",
    "            \n",
    "            if rule_identifier_lift not in rule_frequencies_lift:\n",
    "                rule_frequencies_lift[rule_identifier_lift] = {shop: 1}\n",
    "            else:\n",
    "                if shop in rule_frequencies_lift[rule_identifier_lift]:\n",
    "                    rule_frequencies_lift[rule_identifier_lift][shop] += 1\n",
    "                else:\n",
    "                    rule_frequencies_lift[rule_identifier_lift][shop] = 1\n",
    "\n",
    "# Convert the rule frequencies into a DataFrame for easier manipulation\n",
    "rules_df_lift = pd.DataFrame.from_dict(rule_frequencies_lift, orient='index').fillna(0)\n",
    "rules_df_lift.reset_index(inplace=True)\n",
    "rules_df_lift.rename(columns={'index': 'Rule'}, inplace=True)\n",
    "\n",
    "# Now, each rule string is sorted within itself by the numeric part of the ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6763c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_supports_lift = {}\n",
    "\n",
    "for shop, years_dfs_lift in meta_dfs_lift.items():\n",
    "    for year, df in years_dfs_lift.items():\n",
    "        # Iterate through each row to extract rule support\n",
    "        for _, row in df.iterrows():\n",
    "            # Sort antecedents and consequents\n",
    "            sorted_antecedents = sorted(list(row['antecedents']), key=lambda x: int(x.split('_')[1]))\n",
    "            sorted_consequents = sorted(list(row['consequents']), key=lambda x: int(x.split('_')[1]))\n",
    "            \n",
    "            # Create a unique identifier for each rule\n",
    "            rule_identifier_lift = f\"{tuple(sorted_antecedents)} -> {tuple(sorted_consequents)}\"\n",
    "            \n",
    "            # Initialize or update the support for the rule in the specific shop\n",
    "            rule_supports_lift.setdefault(rule_identifier_lift, {})[shop] = row['support']\n",
    "\n",
    "# Convert the rule supports into a DataFrame for easier manipulation\n",
    "rules_df_lift = pd.DataFrame.from_dict(rule_supports_lift, orient='index').fillna(0)\n",
    "rules_df_lift.reset_index(inplace=True)\n",
    "rules_df_lift.rename(columns={'index': 'Rule'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6fe76e",
   "metadata": {},
   "outputs": [],
   "source": [
    " rule_lifts_lift = {}\n",
    "\n",
    "for shop, years_dfs_lift in meta_dfs_lift.items():\n",
    "    for year, df in years_dfs_lift.items():\n",
    "        # Iterate through each row to extract rule lift\n",
    "        for _, row in df.iterrows():\n",
    "            # Sort antecedents and consequents\n",
    "            sorted_antecedents = sorted(list(row['antecedents']), key=lambda x: int(x.split('_')[1]))\n",
    "            sorted_consequents = sorted(list(row['consequents']), key=lambda x: int(x.split('_')[1]))\n",
    "            \n",
    "            # Create a unique identifier for each rule\n",
    "            rule_identifier_lift = f\"{tuple(sorted_antecedents)} -> {tuple(sorted_consequents)}\"\n",
    "            \n",
    "            # Initialize or update the lift for the rule in the specific shop\n",
    "            rule_lifts_lift.setdefault(rule_identifier_lift, {})[shop] = row['lift']\n",
    "\n",
    "# Convert the rule lifts into a DataFrame for easier manipulation\n",
    "lifts_df_lift = pd.DataFrame.from_dict(rule_lifts_lift, orient='index').fillna(0)\n",
    "lifts_df_lift.reset_index(inplace=True)\n",
    "lifts_df_lift.rename(columns={'index': 'Rule'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05324fe3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Assuming rules_df is already in a suitable format for plotting\n",
    "plt.figure(figsize=(60, 20))\n",
    "sns.heatmap(rules_df_lift.set_index('Rule'), annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
    "plt.title('Rule Support Across Shops')\n",
    "plt.ylabel('Rules')\n",
    "plt.xlabel('Shops')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46fc3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(54, 40))\n",
    "sns.heatmap(rules_df_lift.set_index('Rule'), annot=True, fmt=\".0f\", cmap=\"YlGnBu\")\n",
    "plt.title('Rule Frequencies Across Shops')\n",
    "plt.ylabel('Rules')\n",
    "plt.xlabel('Shops')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744d0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(62, 22))\n",
    "sns.heatmap(lifts_df_lift.set_index('Rule'), annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
    "plt.title('Rule Lift Across Shops')\n",
    "plt.ylabel('Rules')\n",
    "plt.xlabel('Shops')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f42035",
   "metadata": {},
   "source": [
    "# Meta Analysis, Time Series Search 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa8b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by 'rule', 'shop', and 'year'\n",
    "rule_metrics_time_lift = combined_data_lift.groupby(['rule', 'shop', 'year']).agg({\n",
    "    'support': 'mean',\n",
    "    'confidence': 'mean',\n",
    "    'lift': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate the maximum year for each shop and convert it into a DataFrame for merging\n",
    "max_year_per_shop_lift = rule_metrics_time_lift.groupby('shop')['year'].max().reset_index()\n",
    "max_year_per_shop_lift.rename(columns={'year': 'year_max'}, inplace=True)\n",
    "\n",
    "# Merge the maximum year information back into the rule metrics\n",
    "rule_first_last_year_lift = rule_metrics_time_lift.groupby(['rule', 'shop']).agg(\n",
    "    first_year=('year', 'min'),\n",
    "    last_year=('year', 'max')\n",
    ").reset_index()\n",
    "\n",
    "rule_first_last_year_lift = rule_first_last_year_lift.merge(max_year_per_shop_lift, on='shop', how='left')\n",
    "rule_first_last_year_lift['is_current'] = rule_first_last_year_lift['last_year'] == rule_first_last_year_lift['year_max']\n",
    "\n",
    "# Filter to analyze further or visualize trends\n",
    "print(rule_first_last_year_lift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b2fd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out active rules\n",
    "active_rules_lift = rule_first_last_year_lift[rule_first_last_year_lift['is_current']]\n",
    "\n",
    "# Printing active rules for verification\n",
    "print(active_rules_lift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781bd058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge active rules with original metrics to get the full data for these rules\n",
    "active_rule_metrics_lift = active_rules_lift.merge(rule_metrics_time_lift, on=['rule', 'shop'], how='left')\n",
    "\n",
    "# Aggregate to get the mean confidence for each rule across all years it was active\n",
    "active_rule_lift_lift = active_rule_metrics_lift.groupby(['rule', 'shop']).agg({\n",
    "    'lift': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Sorting the results by confidence for better visualization\n",
    "active_rule_lift_sorted_lift = active_rule_lift_lift.sort_values(by='lift', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddc3690",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Setting up the plot\n",
    "plt.figure(figsize=(10, 20))\n",
    "sns.barplot(x='lift', y='rule', data=active_rule_lift_sorted_lift, hue='shop', dodge=False)\n",
    "plt.title('Mean lift of Active Rules by Shop')\n",
    "plt.xlabel('Mean lift')\n",
    "plt.ylabel('Rule')\n",
    "plt.legend(title='Shop')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5125841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'combined_data_lift' is already grouped and aggregated as described earlier\n",
    "\n",
    "# Prepare the growth_decay DataFrame for lift\n",
    "growth_decay_lift = rule_first_last_year_lift.copy()\n",
    "\n",
    "# Calculate growth and decay rates for lift\n",
    "growth_decay_lift['support_change_lift'] = growth_decay_lift.apply(\n",
    "    lambda x: (\n",
    "        rule_metrics_time_lift[(rule_metrics_time_lift['rule'] == x['rule']) & (rule_metrics_time_lift['shop'] == x['shop']) & (rule_metrics_time_lift['year'] == x['last_year'])]['support'].values[0] -\n",
    "        rule_metrics_time_lift[(rule_metrics_time_lift['rule'] == x['rule']) & (rule_metrics_time_lift['shop'] == x['shop']) & (rule_metrics_time_lift['year'] == x['first_year'])]['support'].values[0]\n",
    "    ) / rule_metrics_time_lift[(rule_metrics_time_lift['rule'] == x['rule']) & (rule_metrics_time_lift['shop'] == x['shop']) & (rule_metrics_time_lift['year'] == x['first_year'])]['support'].values[0] if x['first_year'] != x['last_year'] else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "growth_decay_lift['confidence_change_lift'] = growth_decay_lift.apply(\n",
    "    lambda x: (\n",
    "        rule_metrics_time_lift[(rule_metrics_time_lift['rule'] == x['rule']) & (rule_metrics_time_lift['shop'] == x['shop']) & (rule_metrics_time_lift['year'] == x['last_year'])]['confidence'].values[0] -\n",
    "        rule_metrics_time_lift[(rule_metrics_time_lift['rule'] == x['rule']) & (rule_metrics_time_lift['shop'] == x['shop']) & (rule_metrics_time_lift['year'] == x['first_year'])]['confidence'].values[0]\n",
    "    ) / rule_metrics_time_lift[(rule_metrics_time_lift['rule'] == x['rule']) & (rule_metrics_time_lift['shop'] == x['shop']) & (rule_metrics_time_lift['year'] == x['first_year'])]['confidence'].values[0] if x['first_year'] != x['last_year'] else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "growth_decay_lift['lift_change_lift'] = growth_decay_lift.apply(\n",
    "    lambda x: (\n",
    "        rule_metrics_time_lift[(rule_metrics_time_lift['rule'] == x['rule']) & (rule_metrics_time_lift['shop'] == x['shop']) & (rule_metrics_time_lift['year'] == x['last_year'])]['lift'].values[0] -\n",
    "        rule_metrics_time_lift[(rule_metrics_time_lift['rule'] == x['rule']) & (rule_metrics_time_lift['shop'] == x['shop']) & (rule_metrics_time_lift['year'] == x['first_year'])]['lift'].values[0]\n",
    "    ) / rule_metrics_time_lift[(rule_metrics_time_lift['rule'] == x['rule']) & (rule_metrics_time_lift['shop'] == x['shop']) & (rule_metrics_time_lift['year'] == x['first_year'])]['lift'].values[0] if x['first_year'] != x['last_year'] else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Now, filter for significant changes\n",
    "significant_confidence_changes_lift = growth_decay_lift[growth_decay_lift['confidence_change_lift'].abs() > 0.1]\n",
    "significant_lift_changes_lift = growth_decay_lift[growth_decay_lift['lift_change_lift'].abs() > 0.1]\n",
    "significant_support_changes_lift = growth_decay_lift[growth_decay_lift['support_change_lift'].abs() > 0.1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e59cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure your data is sorted if needed (for better visualization), for example by absolute change\n",
    "significant_confidence_changes_lift_sorted = significant_confidence_changes_lift.sort_values('confidence_change_lift', ascending=False)\n",
    "\n",
    "# Visualize significant changes in confidence using the lift-specific data\n",
    "plt.figure(figsize=(14, 8))  # Adjust the figure size to accommodate the number of rules\n",
    "sns.barplot(\n",
    "    x='confidence_change_lift', \n",
    "    y='rule', \n",
    "    data=significant_confidence_changes_lift_sorted, \n",
    "    hue='shop', \n",
    "    dodge=False\n",
    ")\n",
    "plt.title('Significant Confidence Changes in Rules (Lift Analysis)')\n",
    "plt.xlabel('Confidence Change Rate')\n",
    "plt.ylabel('Rule')\n",
    "plt.grid(True)\n",
    "plt.legend(title='Shop')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7e3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure your data is sorted if needed (for better visualization), for example by absolute change\n",
    "significant_lift_changes_lift_sorted = significant_lift_changes_lift.sort_values('lift_change_lift', ascending=False)\n",
    "\n",
    "# Visualize significant changes in lift using the lift-specific data\n",
    "plt.figure(figsize=(10, 8))  # Adjust the figure size if necessary to fit your data\n",
    "sns.barplot(\n",
    "    x='lift_change_lift', \n",
    "    y='rule', \n",
    "    data=significant_lift_changes_lift_sorted, \n",
    "    hue='shop', \n",
    "    dodge=False\n",
    ")\n",
    "plt.title('Significant Lift Changes in Rules')\n",
    "plt.xlabel('Lift Change Rate')\n",
    "plt.ylabel('Rule')\n",
    "plt.grid(True)\n",
    "plt.legend(title='Shop')\n",
    "plt.tight_layout()  # Adjust the layout to fit everything nicely\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d02dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure your data is sorted if needed (for better visualization), for example by absolute change\n",
    "significant_support_changes_lift_sorted = significant_support_changes_lift.sort_values('support_change_lift', ascending=False)\n",
    "\n",
    "# Visualize significant changes in support using the lift-specific data\n",
    "plt.figure(figsize=(10, 8))  # Adjust the figure size if necessary to fit your data\n",
    "sns.barplot(\n",
    "    x='support_change_lift', \n",
    "    y='rule', \n",
    "    data=significant_support_changes_lift_sorted, \n",
    "    hue='shop', \n",
    "    dodge=False\n",
    ")\n",
    "plt.title('Significant Support Changes in Rules (Lift Analysis)')\n",
    "plt.xlabel('Support Change Rate')\n",
    "plt.ylabel('Rule')\n",
    "plt.grid(True)\n",
    "plt.legend(title='Shop')\n",
    "plt.tight_layout()  # Adjust the layout to fit everything nicely\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53134cf3",
   "metadata": {},
   "source": [
    "# Meta Analysis, Network Analysis, Search 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5488cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (e.g., frozensets or lists). If they are not, you will need to preprocess these into individual items.\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Adding nodes and edges from rules\n",
    "for idx, row in combined_data_lift.iterrows():\n",
    "    antecedents = list(row['antecedents'])\n",
    "    consequents = list(row['consequents'])\n",
    "    for antecedent in antecedents:\n",
    "        for consequent in consequents:\n",
    "            if G.has_edge(antecedent, consequent):\n",
    "                # Increase weight by support or confidence\n",
    "                G[antecedent][consequent]['weight'] += row['support']\n",
    "            else:\n",
    "                G.add_edge(antecedent, consequent, weight=row['support'])\n",
    "\n",
    "# This creates a graph where nodes are items and edges are weighted by support.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce5483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate centrality measures\n",
    "degree_centrality_lift = nx.degree_centrality(G)\n",
    "betweenness_centrality_lift = nx.betweenness_centrality(G)\n",
    "closeness_centrality_lift = nx.closeness_centrality(G)\n",
    "\n",
    "# Convert to DataFrame for easier manipulation and visualization\n",
    "centrality_df_lift = pd.DataFrame({\n",
    "    'Node': degree_centrality_lift.keys(),\n",
    "    'Degree Centrality': degree_centrality_lift.values(),\n",
    "    'Betweenness Centrality': betweenness_centrality_lift.values(),\n",
    "    'Closeness Centrality': closeness_centrality_lift.values()\n",
    "}).sort_values(by='Degree Centrality', ascending=False)\n",
    "\n",
    "print(centrality_df_lift.head(10))  # Display top 10 central nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d63c95",
   "metadata": {},
   "source": [
    "## Strategic Implications\n",
    "\n",
    "### Inventory Management: \n",
    "Items represented by larger or brighter nodes should be prioritized in inventory decisions as they have the potential to drive more sales when paired with other items.\n",
    "\n",
    "### Marketing and Promotion: \n",
    "These items are ideal candidates for promotions, as their high connectivity suggests they can drive additional purchases. Marketing campaigns can focus on these items to leverage their central role in purchase patterns.\n",
    "\n",
    "### Product Placement: \n",
    "In physical or digital stores, placing these central items in prominent positions can increase visibility and potentially encourage more basket additions.\n",
    "\n",
    "### Customer Insights: \n",
    "Understanding which items are central can help in understanding customer preferences and behaviors, particularly how different products influence shopping patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f07c249",
   "metadata": {},
   "source": [
    "# Meta Analysis, Aggregated Rules Search 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f911fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each rule across the dataset\n",
    "rule_frequency_lift = combined_data_lift['rule'].value_counts()\n",
    "\n",
    "# Filter rules that appear more than a certain threshold, e.g., appear in more than 5 different combinations of shop and year\n",
    "significant_rules_lift = rule_frequency_lift[rule_frequency_lift > 5]  # Adjust threshold as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d81f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the combined_data to include only significant rules\n",
    "significant_data_lift = combined_data_lift[combined_data_lift['rule'].isin(significant_rules_lift.index)]\n",
    "\n",
    "# Calculate average support, confidence, and lift for these significant rules\n",
    "average_metrics_lift = significant_data_lift.groupby('rule').agg({\n",
    "    'support': 'mean',\n",
    "    'confidence': 'mean',\n",
    "    'lift': 'mean'\n",
    "}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca14ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the significant rules with their average metrics\n",
    "print(average_metrics_lift)\n",
    "\n",
    "# Optionally, save this data to a CSV file\n",
    "average_metrics_lift.to_csv('significant_rule_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c9242",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_metrics_lift.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657b0fe9",
   "metadata": {},
   "source": [
    "# Meta Analysis, Clustering to see differences, Search 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8f070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DataFrame structure\n",
    "data_lift = {\n",
    "    'shop': combined_data_lift['shop'],\n",
    "    'rule': combined_data_lift['antecedents'].astype(str) + '->' + combined_data_lift['consequents'].astype(str),\n",
    "    'lift': combined_data_lift['lift']\n",
    "}\n",
    "\n",
    "df_lift = pd.DataFrame(data_lift)\n",
    "\n",
    "# Pivot table to transform data into a [shops x rules] matrix, filling missing rules with zero\n",
    "feature_matrix_lift = df_lift.pivot_table(index='shop', columns='rule', values='lift', fill_value=0)\n",
    "\n",
    "# Normalize the feature matrix\n",
    "scaler = MinMaxScaler()\n",
    "feature_matrix_normalized_lift = scaler.fit_transform(feature_matrix_lift)\n",
    "feature_matrix_normalized_lift = pd.DataFrame(feature_matrix_normalized_lift, index=feature_matrix_lift.index, columns=feature_matrix_lift.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d2498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix_lift.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c7eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lift.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff777358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal number of clusters using the elbow method\n",
    "sse_lift = {}\n",
    "for k in range(1, 9):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(feature_matrix_normalized_lift)\n",
    "    sse_lift[k] = kmeans.inertia_  # Sum of squared distances of samples to their closest cluster center\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(list(sse_lift.keys()), list(sse_lift.values()))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.title(\"Elbow Method For Optimal k\")\n",
    "plt.show()\n",
    "\n",
    "# Assume the elbow is found at k=4\n",
    "optimal_k_lift = 4\n",
    "kmeans = KMeans(n_clusters=optimal_k_lift, random_state=42)\n",
    "clusters_lift = kmeans.fit_predict(feature_matrix_normalized_lift)\n",
    "\n",
    "# Add cluster information back to the original DataFrame\n",
    "feature_matrix_lift['Cluster'] = clusters_lift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b1cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_lift = PCA(n_components=2)\n",
    "principal_components_lift = pca_lift.fit_transform(feature_matrix_normalized_lift)\n",
    "pca_df_lift = pd.DataFrame(data=principal_components_lift, columns=['PC1_lift', 'PC2_lift'])\n",
    "pca_df_lift['Cluster'] = clusters_lift\n",
    "\n",
    "# Assuming 'pca_df_lift' already has PCA results and cluster labels\n",
    "# Let's add shop identifiers to the DataFrame for easy access\n",
    "pca_df_lift['Shop'] = feature_matrix_lift.index  # This assumes the index of your feature matrix has shop identifiers\n",
    "\n",
    "# Now plotting with annotations\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_lift = sns.scatterplot(x='PC1_lift', y='PC2_lift', hue='Cluster', data=pca_df_lift, palette='viridis', s=100, alpha=0.7)\n",
    "\n",
    "# Adding text annotations for each point\n",
    "for i in range(pca_df_lift.shape[0]):\n",
    "    plt.text(x=pca_df_lift.PC1_lift[i] + 0.02,  # x-coordinate position for text\n",
    "             y=pca_df_lift.PC2_lift[i] + 0.02,  # y-coordinate position for text\n",
    "             s=pca_df_lift.Shop[i],  # text label\n",
    "             fontdict=dict(color='black', size=10),\n",
    "            )\n",
    "\n",
    "plt.title('PCA Cluster Plot of Shops with Shop Labels (Lift)')\n",
    "plt.xlabel('Principal Component 1 (Lift)')\n",
    "plt.ylabel('Principal Component 2 (Lift)')\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e620f3",
   "metadata": {},
   "source": [
    "1. Data Attributes\n",
    "The primary attributes used for clustering were based on association rules, specifically:\n",
    "\n",
    "Rules: Each unique combination of antecedents (items that lead to a purchase) and consequents (items that are purchased as a result) formed a rule.\n",
    "Confidence: The strength of each rule, which measures the reliability of the rule, was used as the key attribute. Confidence is defined as the probability of seeing the consequent items given the antecedent items have been purchased.\n",
    "\n",
    "2. Construction of the Feature Matrix\n",
    "The feature matrix was constructed as follows:\n",
    "\n",
    "Rows: Each row represented a shop.\n",
    "\n",
    "Columns: Each column represented a unique association rule derived from the combined data of all shops. The uniqueness of a rule was based on the specific combination of items in the antecedents and consequents.\n",
    "\n",
    "Values: The value in each cell of the matrix was the confidence level of the rule for that shop. If a shop did not have a particular rule present in its transactions, the confidence was set to zero.\n",
    "\n",
    "3. Data Preparation for Clustering\n",
    "Normalization: The feature matrix was normalized to ensure that each rule was equally weighted during the clustering process. Normalization adjusted the confidence values so that they were scaled between 0 and 1 across the dataset. This step is crucial because it prevents rules with naturally higher confidence levels from disproportionately influencing the clustering.\n",
    "\n",
    "4. Clustering Algorithm\n",
    "K-Means Clustering: This algorithm was chosen for its efficiency and effectiveness in grouping data into clusters that minimize the variance within each cluster. The number of clusters (k) was determined using the elbow method, which identifies a point where adding more clusters does not significantly improve the within-cluster sum of squares (SSE).\n",
    "\n",
    "5. Dimensionality Reduction for Visualization\n",
    "PCA (Principal Component Analysis): Though not directly involved in forming the clusters, PCA was used post-clustering to reduce the high-dimensional feature space into 2 dimensions. This reduction allowed for easy visualization of the clusters and helped interpret how shops are grouped based on the rules' presence and strength.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "The clusters were formed based on how similarly shops behaved concerning the association rules prevalent in their transactions. By using confidence as the clustering attribute, the analysis not only considered which rules were common but also how significant these rules were in each shop's context. This approach provided a nuanced view that combined both the qualitative aspect (which rules are present) and the quantitative aspect (how strong these rules are), leading to a comprehensive grouping based on purchasing patterns. This method is particularly useful for understanding market dynamics, tailoring marketing strategies, and optimizing inventory management based on consumer behavior similarities across shops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3947d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Suppress specific DeprecationWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, message=\"is_sparse is deprecated\")\n",
    "\n",
    "# Example DataFrame structure\n",
    "data_lift = {\n",
    "    'shop_lift': combined_data_lift['shop'],\n",
    "    'rule_lift': combined_data_lift['antecedents'].astype(str) + '->' + combined_data_lift['consequents'].astype(str),\n",
    "    'lift_lift': combined_data_lift['lift']\n",
    "}\n",
    "\n",
    "df_lift = pd.DataFrame(data_lift)\n",
    "\n",
    "# Pivot table to transform data into a [shops x rules] matrix, filling missing rules with zero\n",
    "feature_matrix_lift = df_lift.pivot_table(index='shop_lift', columns='rule_lift', values='lift_lift', fill_value=0)\n",
    "\n",
    "# Normalize the feature matrix\n",
    "scaler_lift = MinMaxScaler()\n",
    "feature_matrix_normalized_lift = scaler_lift.fit_transform(feature_matrix_lift)\n",
    "feature_matrix_normalized_lift = pd.DataFrame(feature_matrix_normalized_lift, index=feature_matrix_lift.index, columns=feature_matrix_lift.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a3ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix_lift.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75519232",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lift.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fbc385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal number of clusters using the elbow method\n",
    "sse_lift = {}\n",
    "for k in range(1, 9):\n",
    "    kmeans_lift = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans_lift.fit(feature_matrix_normalized_lift)\n",
    "    sse_lift[k] = kmeans_lift.inertia_  # Sum of squared distances of samples to their closest cluster center\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(list(sse_lift.keys()), list(sse_lift.values()))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.title(\"Elbow Method For Optimal k (Lift)\")\n",
    "plt.show()\n",
    "\n",
    "# Assume the elbow is found at k=3\n",
    "optimal_k_lift = 3\n",
    "kmeans_lift = KMeans(n_clusters=optimal_k_lift, random_state=42)\n",
    "clusters_lift = kmeans_lift.fit_predict(feature_matrix_normalized_lift)\n",
    "\n",
    "# Add cluster information back to the original DataFrame\n",
    "feature_matrix_lift['Cluster_lift'] = clusters_lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5348bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'feature_matrix_normalized_lift' and 'clusters_lift' are already defined\n",
    "\n",
    "# Perform PCA\n",
    "pca_lift = PCA(n_components=2)\n",
    "principal_components_lift = pca_lift.fit_transform(feature_matrix_normalized_lift)\n",
    "pca_df_lift = pd.DataFrame(data=principal_components_lift, columns=['PC1_lift', 'PC2_lift'])\n",
    "pca_df_lift['Cluster'] = clusters_lift\n",
    "\n",
    "# Assuming 'pca_df_lift' already has PCA results and cluster labels\n",
    "# Let's add shop identifiers to the DataFrame for easy access\n",
    "pca_df_lift['Shop'] = feature_matrix_lift.index  # This assumes the index of your feature matrix has shop identifiers\n",
    "\n",
    "# Now plotting with annotations\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_lift = sns.scatterplot(x='PC1_lift', y='PC2_lift', hue='Cluster', data=pca_df_lift, palette='viridis', s=100, alpha=0.7)\n",
    "\n",
    "# Adding text annotations for each point\n",
    "for i in range(pca_df_lift.shape[0]):\n",
    "    plt.text(x=pca_df_lift.PC1_lift[i] + 0.02,  # x-coordinate position for text\n",
    "             y=pca_df_lift.PC2_lift[i] + 0.02,  # y-coordinate position for text\n",
    "             s=pca_df_lift.Shop[i],  # text label\n",
    "             fontdict=dict(color='black', size=10),\n",
    "            )\n",
    "\n",
    "plt.title('PCA Cluster Plot of Shops with Shop Labels (Lift)')\n",
    "plt.xlabel('Principal Component 1 (Lift)')\n",
    "plt.ylabel('Principal Component 2 (Lift)')\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1d45c5",
   "metadata": {},
   "source": [
    "# Meta Analysis Hierarchical Clustering Search 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc9a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom rule representation using tuples of sorted antecedents and consequents\n",
    "def create_rule(antecedents, consequents):\n",
    "    return (tuple(sorted(antecedents)), tuple(sorted(consequents)))\n",
    "\n",
    "# Create a dictionary to accumulate rules for each shop\n",
    "shops_lift = combined_data_lift['shop'].unique()\n",
    "shop_rules_lift = {shop: set() for shop in shops_lift}\n",
    "\n",
    "for _, row in combined_data_lift.iterrows():\n",
    "    rule_lift = create_rule(row['antecedents'], row['consequents'])\n",
    "    shop_rules_lift[row['shop']].add(rule_lift)\n",
    "\n",
    "# Check rules for Shop11 in shop_rules_lift\n",
    "print(f\"Rules for Shop11 in shop_rules_lift:\\n{shop_rules_lift['shop11']}\")\n",
    "print(f\"Number of unique rules for Shop11 in shop_rules_lift: {len(shop_rules_lift['shop11'])}\")\n",
    "\n",
    "# Create a binary matrix where each row represents a shop and each column a unique rule\n",
    "all_rules_lift = list(set.union(*shop_rules_lift.values()))  # Convert the set to a list\n",
    "rule_matrix_lift = pd.DataFrame(0, index=shops_lift, columns=all_rules_lift, dtype=int)\n",
    "\n",
    "for shop in shops_lift:\n",
    "    for rule_lift in shop_rules_lift[shop]:\n",
    "        rule_matrix_lift.at[shop, rule_lift] = 1\n",
    "\n",
    "# Convert DataFrame to numpy array for distance calculation\n",
    "rule_matrix_np_lift = rule_matrix_lift.to_numpy()\n",
    "\n",
    "# Compute the similarity matrix using Jaccard similarity\n",
    "similarity_matrix_lift = 1 - pairwise_distances(rule_matrix_np_lift, metric='jaccard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77b91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize rule_matrix for hierarchical clustering\n",
    "scaler = MinMaxScaler()\n",
    "rule_matrix_scaled_lift = scaler.fit_transform(rule_matrix_lift)\n",
    "\n",
    "# Hierarchical Clustering\n",
    "hclust_lift = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n",
    "hclusters_lift = hclust_lift.fit_predict(rule_matrix_scaled_lift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb2eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the linkage matrix\n",
    "Z = linkage(rule_matrix_scaled_lift, method='ward')\n",
    "\n",
    "# Plotting the dendrogram\n",
    "plt.figure(figsize=(15, 10))\n",
    "dendrogram(Z, labels=shops, leaf_rotation=90, leaf_font_size=12, color_threshold=0)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Shop')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ade399",
   "metadata": {},
   "source": [
    "Here MDS is applied directly to the data and not to the PCA transformed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a64c38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Defining a list of shop identifiers\n",
    "sorted_shops_lift = ['shop1', 'shop11', 'shop12', 'shop13', 'shop3', 'shop4', 'shop5', 'shop6', 'shop7', 'shop8', 'shop9']\n",
    "print(\"Original order of sorted_shops_lift:\")\n",
    "print(sorted_shops_lift)\n",
    "\n",
    "# Function to extract numbers from shop identifiers\n",
    "def extract_number(text):\n",
    "    num = re.search(r'\\d+', text)\n",
    "    return int(num.group()) if num else None\n",
    "\n",
    "# Sort the list by extracting numbers and sorting based on them\n",
    "sorted_shops_lift = sorted(sorted_shops_lift, key=extract_number)\n",
    "\n",
    "print(\"\\nNumerically sorted sorted_shops_lift:\")\n",
    "print(sorted_shops_lift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f39df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to create a sorted shop list if not already available\n",
    "\n",
    "# Convert hclusters_lift to a pandas Series with shops as the index\n",
    "hclusters_series = pd.Series(hclusters_lift, index=sorted_shops_lift)\n",
    "\n",
    "# Reindex hclusters_series to match pca_df_lift\n",
    "# This ensures that cluster labels are aligned with the PCA data\n",
    "hclusters_aligned = hclusters_series.reindex(pca_df_lift['Shop']).values\n",
    "\n",
    "# Assign the correctly ordered clusters to pca_df_lift\n",
    "pca_df_lift['Cluster'] = hclusters_aligned\n",
    "\n",
    "# Now plotting with annotations\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_lift = sns.scatterplot(x='PC1_lift', y='PC2_lift', hue='Cluster', data=pca_df_lift, palette='viridis', s=100, alpha=0.7)\n",
    "\n",
    "# Adding text annotations for each point\n",
    "for i in range(pca_df_lift.shape[0]):\n",
    "    plt.text(x=pca_df_lift.PC1_lift[i] + 0.02,  # x-coordinate position for text\n",
    "             y=pca_df_lift.PC2_lift[i] + 0.02,  # y-coordinate position for text\n",
    "             s=pca_df_lift.Shop[i],  # text label\n",
    "             fontdict=dict(color='black', size=10),\n",
    "            )\n",
    "\n",
    "plt.title('PCA Hierarchical Cluster Plot of Shops with Shop Labels (Lift)')\n",
    "plt.xlabel('Principal Component 1 (Lift)')\n",
    "plt.ylabel('Principal Component 2 (Lift)')\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5623fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom rule representation using tuples of sorted antecedents and consequents\n",
    "def create_rule(antecedents, consequents):\n",
    "    return (tuple(sorted(antecedents)), tuple(sorted(consequents)))\n",
    "\n",
    "# Create a dictionary to accumulate rules for each shop\n",
    "shops_lift = combined_data_lift['shop'].unique()\n",
    "shop_rules_lift = {shop: set() for shop in shops_lift}\n",
    "\n",
    "for _, row in combined_data_lift.iterrows():\n",
    "    rule_lift = create_rule(row['antecedents'], row['consequents'])\n",
    "    shop_rules_lift[row['shop']].add(rule_lift)\n",
    "\n",
    "# Create a binary matrix where each row represents a shop and each column a unique rule\n",
    "all_rules_lift = list(set.union(*shop_rules_lift.values()))  # Convert the set to a list\n",
    "rule_matrix_lift = pd.DataFrame(0, index=shops_lift, columns=all_rules_lift, dtype=int)\n",
    "\n",
    "for shop in shops_lift:\n",
    "    for rule_lift in shop_rules_lift[shop]:\n",
    "        rule_matrix_lift.at[shop, rule_lift] = 1\n",
    "\n",
    "# Convert DataFrame to numpy array for distance calculation\n",
    "rule_matrix_np_lift = rule_matrix_lift.to_numpy()\n",
    "\n",
    "# Compute the similarity matrix using Jaccard similarity\n",
    "similarity_matrix_lift = 1 - pairwise_distances(rule_matrix_np_lift, metric='jaccard')\n",
    "\n",
    "# Apply MDS\n",
    "mds_lift = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "shop_positions_lift = mds_lift.fit_transform(similarity_matrix_lift)  # Already 1 - similarity_matrix gives dissimilarity\n",
    "\n",
    "# Prepare results for plotting\n",
    "shop_positions_df_lift = pd.DataFrame(shop_positions_lift, index=shops_lift, columns=['x', 'y'])\n",
    "\n",
    "# Use the cluster information from pca_df_lift\n",
    "shop_positions_df_lift['MDS Cluster'] = pca_df_lift.set_index('Shop').reindex(shops_lift)['Cluster']\n",
    "\n",
    "# Create the network graph\n",
    "G_lift = nx.Graph()\n",
    "\n",
    "# Add all nodes for each shop\n",
    "for shop_lift in shops_lift:\n",
    "    G_lift.add_node(shop_lift)\n",
    "\n",
    "# Connect all shops with edges weighted by similarity\n",
    "for i, shop_i_lift in enumerate(shops_lift):\n",
    "    for j, shop_j_lift in enumerate(shops_lift):\n",
    "        if i < j:  # Avoid self-loops and duplicate edges\n",
    "            similarity_lift = similarity_matrix_lift[i, j]\n",
    "            # Use a continuous scale for edge width\n",
    "            G_lift.add_edge(shop_i_lift, shop_j_lift, weight=similarity_lift)\n",
    "\n",
    "# Calculate the number of rules per shop for node sizes\n",
    "num_rules_per_shop_lift = {shop_lift: len(rules) for shop_lift, rules in shop_rules_lift.items()}\n",
    "\n",
    "# Normalize the values to use for node size\n",
    "sizes_lift = [num_rules_per_shop_lift[shop_lift] for shop_lift in G_lift.nodes()]\n",
    "max_size_lift = max(sizes_lift)\n",
    "min_size_lift = min(sizes_lift)\n",
    "node_sizes_lift = [(size_lift - min_size_lift) / (max_size_lift - min_size_lift) * 1000 + 100 for size_lift in sizes_lift]  # Scale between 100 and 1100\n",
    "\n",
    "# Use the 'MDS Cluster' for coloring nodes\n",
    "cluster_colors_lift = shop_positions_df_lift['MDS Cluster']\n",
    "\n",
    "# If the cluster numbers are not sequential starting from 0, map them to a sequential range\n",
    "unique_clusters_lift = sorted(cluster_colors_lift.unique())\n",
    "cluster_color_mapping_lift = {cluster_lift: i for i, cluster_lift in enumerate(unique_clusters_lift)}\n",
    "sequential_cluster_colors_lift = cluster_colors_lift.map(cluster_color_mapping_lift)\n",
    "\n",
    "# Generate a color palette with seaborn and map the clusters to colors using viridis\n",
    "palette_lift = sns.color_palette('viridis', len(unique_clusters_lift))\n",
    "node_colors_lift = [palette_lift[color_lift] for color_lift in sequential_cluster_colors_lift]\n",
    "\n",
    "# Draw the network with the updated color palette\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos_lift = nx.spring_layout(G_lift, seed=42)  # Positions should be consistent with previous plot\n",
    "\n",
    "# Draw nodes with sizes based on the number of rules and color based on clusters using the viridis palette\n",
    "nodes_lift = nx.draw_networkx_nodes(G_lift, pos_lift, node_size=node_sizes_lift, node_color=node_colors_lift, alpha=0.7)\n",
    "\n",
    "# Draw edges\n",
    "edges_lift = nx.draw_networkx_edges(G_lift, pos_lift, width=[G_lift[u][v]['weight']*5 for u, v in G_lift.edges()], alpha=0.5)\n",
    "\n",
    "# Draw labels\n",
    "labels_lift = nx.draw_networkx_labels(G_lift, pos_lift, font_size=10)\n",
    "\n",
    "plt.title('Network of Shops Based on Rule Similarities (Colored by Hierarchical Clusters) - Lift')\n",
    "plt.axis('off')  # Turn off the axis if not needed\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0126c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'feature_matrix_normalized_lift' and 'shops_lift' are already defined\n",
    "# K-means clustering has been performed with the determined optimal number of clusters\n",
    "kmeans_lift = KMeans(n_clusters=3, random_state=42)  # Change the number of clusters based on your elbow method results\n",
    "clusters_lift = kmeans_lift.fit_predict(feature_matrix_normalized_lift)\n",
    "\n",
    "# Add cluster labels back to the original DataFrame for use in plotting\n",
    "feature_matrix_lift['Cluster'] = clusters_lift  # Assumes feature_matrix_lift is a DataFrame with shops as rows\n",
    "\n",
    "# Create a graph\n",
    "G_lift = nx.Graph()\n",
    "\n",
    "# Add all nodes for each shop, including cluster info for coloring\n",
    "for shop_lift, data in feature_matrix_lift.iterrows():\n",
    "    G_lift.add_node(shop_lift, cluster=data['Cluster'])\n",
    "\n",
    "# Connect all shops with edges weighted by similarity\n",
    "for i, shop_i_lift in enumerate(shops_lift):\n",
    "    for j, shop_j_lift in enumerate(shops_lift):\n",
    "        if i < j:  # Avoid self-loops and duplicate edges\n",
    "            similarity_lift = similarity_matrix_lift[i, j]\n",
    "            G_lift.add_edge(shop_i_lift, shop_j_lift, weight=similarity_lift)\n",
    "\n",
    "# Calculate node sizes based on the number of unique rules compared to the other shops in their cluster\n",
    "unique_rules_per_shop_lift = {}\n",
    "for shop_lift in shops_lift:\n",
    "    cluster = feature_matrix_lift.at[shop_lift, 'Cluster']\n",
    "    shops_in_cluster_lift = feature_matrix_lift[feature_matrix_lift['Cluster'] == cluster].index\n",
    "    all_rules_in_cluster_lift = set()\n",
    "    for shop in shops_in_cluster_lift:\n",
    "        all_rules_in_cluster_lift.update(shop_rules_lift[shop])\n",
    "    unique_rules_per_shop_lift[shop_lift] = len((all_rules_in_cluster_lift) - set(shop_rules_lift[shop])) / len(all_rules_in_cluster_lift)\n",
    "\n",
    "sizes_lift = [unique_rules_per_shop_lift[shop_lift] for shop_lift in G_lift.nodes()]\n",
    "max_size_lift = max(sizes_lift)\n",
    "min_size_lift = min(sizes_lift)\n",
    "node_sizes_lift = [(size_lift - min_size_lift) / (max_size_lift - min_size_lift) * 2500 + 100 for size_lift in sizes_lift]\n",
    "\n",
    "# Use cluster information for coloring nodes\n",
    "cluster_colors_lift = [G_lift.nodes[shop_lift]['cluster'] for shop_lift in G_lift.nodes()]\n",
    "\n",
    "# Draw the network\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos_lift = nx.spring_layout(G_lift, seed=42)\n",
    "nx.draw_networkx_nodes(G_lift, pos_lift, node_size=node_sizes_lift, node_color=cluster_colors_lift, cmap='viridis', alpha=0.7)\n",
    "nx.draw_networkx_edges(G_lift, pos_lift, width=[G_lift[u][v]['weight']*5 for u, v in G_lift.edges()], alpha=0.5)\n",
    "nx.draw_networkx_labels(G_lift, pos_lift, font_size=10)\n",
    "plt.title('Network of Shops Based on Rule Similarities (Colored by K-means Clusters)')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee54c732",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_rules_in_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2bd267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate node sizes based on the number of unique rules compared to the other shops in their cluster\n",
    "unique_rules_per_shop_lift = {}\n",
    "excluded_shop = 'shop9'  # Shop to be excluded\n",
    "\n",
    "for shop in shops:\n",
    "    if shop == excluded_shop:\n",
    "        continue\n",
    "    \n",
    "    cluster_lift = feature_matrix_lift.at[shop, 'Cluster']\n",
    "    shops_in_cluster_lift = feature_matrix_lift[feature_matrix_lift['Cluster'] == cluster_lift].index\n",
    "    all_rules_in_cluster_lift = set()\n",
    "    \n",
    "    for shop_in_cluster_lift in shops_in_cluster_lift:\n",
    "        if shop_in_cluster_lift != excluded_shop:\n",
    "            all_rules_in_cluster_lift.update(shop_rules_lift[shop_in_cluster_lift])\n",
    "    \n",
    "    unique_rules_per_shop_lift[shop] = len((all_rules_in_cluster_lift) - set(shop_rules_lift[shop])) / len(all_rules_in_cluster_lift)\n",
    "\n",
    "# Print the unique rules proportion for each shop\n",
    "for shop, value in unique_rules_per_shop_lift.items():\n",
    "    print(f\"Shop: {shop}\\nUnique Rules Proportion: {value:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa89b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda7cc17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ae6783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
