{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51ea2b45",
   "metadata": {},
   "source": [
    "# Read me\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a717673b",
   "metadata": {},
   "source": [
    "## please run the Exploratory and Association Rules Analysis first. This code requires pickles from the Association Rule Mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1e0b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-posthocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff845d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import ticker\n",
    "import joypy\n",
    "import os\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import KMeans\n",
    "from math import ceil\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import networkx as nx\n",
    "import matplotlib.colors as mcolors\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, leaves_list\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "from scipy.cluster.hierarchy import linkage as sch_linkage\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import scipy.stats as stats\n",
    "from collections import defaultdict\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import warnings\n",
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719bfa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5101a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary from the file\n",
    "with open('meta_dfs.pkl', 'rb') as file:\n",
    "    meta_dfs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5f51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary from the file\n",
    "with open('meta_dfs_redundant.pkl', 'rb') as file:\n",
    "    meta_dfs_redundant = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c30ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data = pd.read_csv('anon_sales.csv')\n",
    "sales_data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f349e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and file path\n",
    "directory = os.path.expanduser(\"~/Desktop/Master Project newest/data\")\n",
    "file_path = os.path.join(directory, \"combined_scores.csv\")\n",
    "\n",
    "# Read the CSV file\n",
    "best_rules = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d09aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'sub_family_' with '' (empty string) to leave only the number part\n",
    "sales_data['family'] = sales_data['family'].str.replace('family_', '')\n",
    "\n",
    "# Optionally, if you want to convert the column to numeric type\n",
    "sales_data['family'] = pd.to_numeric(sales_data['family'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b697e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'sub_family_' with '' (empty string) to leave only the number part\n",
    "sales_data['sub_family'] = sales_data['sub_family'].str.replace('sub_family_', '')\n",
    "\n",
    "# Optionally, if you want to convert the column to numeric type\n",
    "sales_data['sub_family'] = pd.to_numeric(sales_data['sub_family'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08957791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'sub_family_' with '' (empty string) to leave only the number part\n",
    "sales_data['shopID'] = sales_data['shopID'].str.replace('ShopID_', '')\n",
    "\n",
    "# Optionally, if you want to convert the column to numeric type\n",
    "sales_data['shopID'] = pd.to_numeric(sales_data['shopID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'sub_family_' with '' (empty string) to leave only the number part\n",
    "sales_data['product_code'] = sales_data['product_code'].str.replace('Product_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079ff0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'trx_id' with '' (empty string) to leave only the number part\n",
    "sales_data['trx_id'] = sales_data['trx_id'].str.replace('trxID_', '')\n",
    "\n",
    "# Optionally, if you want to convert the column to numeric type\n",
    "sales_data['trx_id'] = pd.to_numeric(sales_data['trx_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8e0531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'trx_date' column to datetime format\n",
    "sales_data['trx_date'] = pd.to_datetime(sales_data['trx_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f4c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data['unit_price'] = pd.to_numeric(sales_data['unit_price'].str.replace(',', '.'), errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c814504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new variable total_value as the product of quantity and unit_price\n",
    "sales_data['total_value'] = sales_data['quantity'] * sales_data['unit_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d31317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from original years to new years\n",
    "year_mapping = {\n",
    "    1992: 2020,\n",
    "    1993: 2021,\n",
    "    1994: 2022,\n",
    "    1995: 2023,\n",
    "    1996: 2024\n",
    "}\n",
    "\n",
    "# Apply the mapping to change the year, keeping month and day the same\n",
    "sales_data['trx_date'] = sales_data['trx_date'].apply(\n",
    "    lambda x: x.replace(year=year_mapping[x.year])\n",
    ")\n",
    "\n",
    "sales_data['trx_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b0d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'year' column based on 'trx_date'\n",
    "sales_data['year'] = sales_data['trx_date'].dt.to_period('Y')\n",
    "\n",
    "# Create a 'quarter' column based on 'trx_date'\n",
    "sales_data['quarter'] = sales_data['trx_date'].dt.to_period('Q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2da46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'year_integer' column by extracting year and converting to integer\n",
    "sales_data['year_integer'] = sales_data['trx_date'].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df5ec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine season\n",
    "def get_season(month):\n",
    "    seasons = {'winter': (12, 1, 2), 'spring': (3, 4, 5), 'summer': (6, 7, 8), 'autumn': (9, 10, 11)}\n",
    "    for season, months in seasons.items():\n",
    "        if month in months:\n",
    "            return season\n",
    "    return None\n",
    "\n",
    "sales_data['season'] = sales_data['trx_date'].dt.month.apply(get_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b837c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day of the week (Monday=1, Tuesday=2, ...)\n",
    "sales_data['day_of_week_number'] = sales_data['trx_date'].dt.dayofweek + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dacb0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day of the week name\n",
    "sales_data['day_of_week_name'] = sales_data['trx_date'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d97baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data['year_month'] = sales_data['trx_date'].dt.to_period('M')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb3846d",
   "metadata": {},
   "source": [
    "# Hypothesis 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa593e6",
   "metadata": {},
   "source": [
    "## There are significant variations in profitability across seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45187743",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract data for each season\n",
    "winter_data = sales_data[sales_data['season'] == 'winter']['total_value']\n",
    "spring_data = sales_data[sales_data['season'] == 'spring']['total_value']\n",
    "summer_data = sales_data[sales_data['season'] == 'summer']['total_value']\n",
    "autumn_data = sales_data[sales_data['season'] == 'autumn']['total_value']\n",
    "\n",
    "# Ensure there are enough data points in each season\n",
    "print(f\"Spring data count: {len(spring_data)}\")\n",
    "print(f\"Summer data count: {len(summer_data)}\")\n",
    "print(f\"Autumn data count: {len(autumn_data)}\")\n",
    "print(f\"Winter data count: {len(winter_data)}\")\n",
    "\n",
    "# Calculate the mean values for sanity check\n",
    "mean_values = sales_data.groupby('season')['total_value'].agg(['mean', 'median', 'max', 'sum', 'count'])\n",
    "print(mean_values)\n",
    "\n",
    "# Function to perform Kolmogorov-Smirnov test\n",
    "def ks_test(data, season):\n",
    "    stat, p_value = stats.kstest(data, 'norm', args=(data.mean(), data.std()))\n",
    "    print(f\"Kolmogorov-Smirnov test for {season}:\")\n",
    "    print(f\"  Test statistic: {stat:.6f}\")\n",
    "    print(f\"  p-value: {p_value:.20f}\")\n",
    "    if p_value > 0.05:\n",
    "        print(f\"  {season} data is normally distributed.\\n\")\n",
    "    else:\n",
    "        print(f\"  {season} data is not normally distributed.\\n\")\n",
    "\n",
    "# Perform Kolmogorov-Smirnov test for each season\n",
    "ks_test(winter_data, 'Winter')\n",
    "ks_test(spring_data, 'Spring')\n",
    "ks_test(summer_data, 'Summer')\n",
    "ks_test(autumn_data, 'Autumn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8939035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Kruskal-Wallis Test\n",
    "kruskal_test = stats.kruskal(winter_data, spring_data, summer_data, autumn_data)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Kruskal-Wallis test statistic: {kruskal_test.statistic:.6f}\")\n",
    "print(f\"Kruskal-Wallis test p-value: {kruskal_test.pvalue:.30f}\")\n",
    "\n",
    "if kruskal_test.pvalue < 0.05:\n",
    "    print(\"There are significant variations in profitability across the seasons.\")\n",
    "else:\n",
    "    print(\"There are no significant variations in profitability across the seasons.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f4ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = sales_data[['total_value', 'season']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ec3cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tukey_oneway = pairwise_tukeyhsd(endog = final_data[\"total_value\"], groups = final_data[\"season\"])\n",
    "\n",
    "# Display the results\n",
    "tukey_oneway.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a645aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for each season\n",
    "seasons = ['winter', 'spring', 'summer', 'autumn']\n",
    "data = {season: sales_data[sales_data['season'] == season]['total_value'] for season in seasons}\n",
    "\n",
    "# Combine the data into a single DataFrame\n",
    "final_data = pd.concat([pd.DataFrame({'season': season, 'total_value': values}) for season, values in data.items()])\n",
    "\n",
    "# Perform Dunn's post hoc test\n",
    "dunn_test = sp.posthoc_dunn(final_data, val_col='total_value', group_col='season', p_adjust='bonferroni')\n",
    "print(dunn_test)\n",
    "\n",
    "# Calculate mean ranks\n",
    "mean_ranks = final_data.groupby('season')['total_value'].mean().sort_values()\n",
    "print(mean_ranks)\n",
    "\n",
    "# Annotate directions on the Dunn's test results\n",
    "direction_matrix = pd.DataFrame(index=seasons, columns=seasons)\n",
    "for i in seasons:\n",
    "    for j in seasons:\n",
    "        if dunn_test.loc[i, j] < 0.05:  # Significant difference\n",
    "            if mean_ranks[i] > mean_ranks[j]:\n",
    "                direction_matrix.loc[i, j] = f\"{i} > {j}\"\n",
    "            else:\n",
    "                direction_matrix.loc[i, j] = f\"{i} < {j}\"\n",
    "        else:\n",
    "            direction_matrix.loc[i, j] = 'No significant difference'\n",
    "\n",
    "print(\"Direction of Differences:\")\n",
    "print(direction_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2efe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e437a",
   "metadata": {},
   "source": [
    "# H1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a06ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for each weekday\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "data = {day: sales_data[sales_data['day_of_week_name'] == day]['total_value'] for day in weekdays}\n",
    "\n",
    "# Combine the data into a single DataFrame\n",
    "final_data = pd.concat([pd.DataFrame({'day_of_week_name': day, 'total_value': values}) for day, values in data.items()])\n",
    "\n",
    "# Perform Kruskal-Wallis Test\n",
    "kruskal_test = stats.kruskal(*data.values())\n",
    "print(f\"Kruskal-Wallis test statistic: {kruskal_test.statistic:.6f}\")\n",
    "print(f\"Kruskal-Wallis test p-value: {kruskal_test.pvalue:.30f}\")\n",
    "\n",
    "if kruskal_test.pvalue < 0.05:\n",
    "    print(\"There are significant variations in profitability across the weekdays.\")\n",
    "    \n",
    "    # Perform Dunn's post hoc test\n",
    "    dunn_test = sp.posthoc_dunn(final_data, val_col='total_value', group_col='day_of_week_name', p_adjust='bonferroni')\n",
    "    print(dunn_test)\n",
    "\n",
    "    # Calculate mean ranks\n",
    "    mean_ranks = final_data.groupby('day_of_week_name')['total_value'].mean().sort_values()\n",
    "    print(mean_ranks)\n",
    "\n",
    "    # Annotate directions on the Dunn's test results\n",
    "    direction_matrix = pd.DataFrame(index=weekdays, columns=weekdays)\n",
    "    for i in weekdays:\n",
    "        for j in weekdays:\n",
    "            if dunn_test.loc[i, j] < 0.05:  # Significant difference\n",
    "                if mean_ranks[i] > mean_ranks[j]:\n",
    "                    direction_matrix.loc[i, j] = f\"{i} > {j}\"\n",
    "                else:\n",
    "                    direction_matrix.loc[i, j] = f\"{i} < {j}\"\n",
    "            else:\n",
    "                direction_matrix.loc[i, j] = 'No significant difference'\n",
    "\n",
    "    print(\"Direction of Differences:\")\n",
    "    print(direction_matrix)\n",
    "else:\n",
    "    print(\"There are no significant variations in profitability across the weekdays.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1875502c",
   "metadata": {},
   "source": [
    "# H1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4cab70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filter the data for the year 2023\n",
    "sales_data_2023 = sales_data[sales_data['year_integer'] == 2023]\n",
    "\n",
    "# Extract data for each month in 2023\n",
    "months = sales_data_2023['year_month'].unique()\n",
    "data = {month: sales_data_2023[sales_data_2023['year_month'] == month]['total_value'] for month in months}\n",
    "\n",
    "# Combine the data into a single DataFrame\n",
    "final_data = pd.concat([pd.DataFrame({'year_month': month, 'total_value': values}) for month, values in data.items()])\n",
    "\n",
    "# Perform Kruskal-Wallis Test\n",
    "kruskal_test = stats.kruskal(*data.values())\n",
    "print(f\"Kruskal-Wallis test statistic: {kruskal_test.statistic:.6f}\")\n",
    "print(f\"Kruskal-Wallis test p-value: {kruskal_test.pvalue:.30f}\")\n",
    "\n",
    "if kruskal_test.pvalue < 0.05:\n",
    "    print(\"There are significant variations in profitability across the months in 2023.\")\n",
    "    \n",
    "    # Perform Dunn's post hoc test\n",
    "    dunn_test = sp.posthoc_dunn(final_data, val_col='total_value', group_col='year_month', p_adjust='bonferroni')\n",
    "    print(dunn_test)\n",
    "\n",
    "    # Calculate mean ranks\n",
    "    mean_ranks = final_data.groupby('year_month')['total_value'].mean().sort_values()\n",
    "    print(mean_ranks)\n",
    "\n",
    "    # Annotate directions on the Dunn's test results\n",
    "    direction_matrix = pd.DataFrame(index=months, columns=months)\n",
    "    for i in months:\n",
    "        for j in months:\n",
    "            if dunn_test.loc[i, j] < 0.05:  # Significant difference\n",
    "                if mean_ranks[i] > mean_ranks[j]:\n",
    "                    direction_matrix.loc[i, j] = f\"{i} > {j}\"\n",
    "                else:\n",
    "                    direction_matrix.loc[i, j] = f\"{i} < {j}\"\n",
    "            else:\n",
    "                direction_matrix.loc[i, j] = 'No significant difference'\n",
    "\n",
    "    print(\"Direction of Differences:\")\n",
    "    print(direction_matrix)\n",
    "else:\n",
    "    print(\"There are no significant variations in profitability across the months in 2023.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f277213",
   "metadata": {},
   "source": [
    "# Hypotheses 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88510c53",
   "metadata": {},
   "source": [
    "## Support, Confidence and Lift show Seasonal Patterns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9b4991",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Ensure the 'trx_date' column is in datetime format\n",
    "sales_data['trx_date'] = pd.to_datetime(sales_data['trx_date'])\n",
    "\n",
    "# Function to assign seasons\n",
    "def assign_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'summer'\n",
    "    else:\n",
    "        return 'autumn'\n",
    "\n",
    "# Assign seasons\n",
    "sales_data['season'] = sales_data['trx_date'].dt.month.apply(assign_season)\n",
    "\n",
    "# Initialize lists to hold all the results\n",
    "all_confidences = []\n",
    "all_supports = []\n",
    "all_lifts = []\n",
    "\n",
    "# Loop through each shop and each season to generate association rules\n",
    "for shop in sales_data['shopID'].unique():\n",
    "    for year in sales_data['year_integer'].unique():\n",
    "        for season in ['spring', 'summer', 'autumn', 'winter']:\n",
    "            # Filter data for the specific shop, year, and season\n",
    "            shop_season_data = sales_data[(sales_data['shopID'] == shop) & \n",
    "                                          (sales_data['year_integer'] == year) & \n",
    "                                          (sales_data['season'] == season)]\n",
    "            \n",
    "            # If there's not enough data, skip\n",
    "            if shop_season_data.empty:\n",
    "                continue\n",
    "            \n",
    "            # Create a basket dataframe\n",
    "            basket = shop_season_data.groupby(['trx_id', 'product_code'])['quantity'].sum().unstack().fillna(0)\n",
    "            basket = basket.map(lambda x: x > 0).astype(bool)  # Convert to boolean matrix\n",
    "            \n",
    "            # Generate frequent itemsets\n",
    "            frequent_itemsets = apriori(basket, min_support=0.001, use_colnames=True)\n",
    "            \n",
    "            # Generate the association rules\n",
    "            rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.4)\n",
    "            \n",
    "            # Add the mean confidence, support, and lift for this shop and season to the lists\n",
    "            if not rules.empty:\n",
    "                mean_confidence = rules['confidence'].mean()\n",
    "                mean_support = rules['support'].mean()\n",
    "                mean_lift = rules['lift'].mean()\n",
    "                all_confidences.append({'shop': shop, 'year': year, 'season': season, 'mean_confidence': mean_confidence})\n",
    "                all_supports.append({'shop': shop, 'year': year, 'season': season, 'mean_support': mean_support})\n",
    "                all_lifts.append({'shop': shop, 'year': year, 'season': season, 'mean_lift': mean_lift})\n",
    "\n",
    "# Convert the lists to DataFrames\n",
    "confidences_df = pd.DataFrame(all_confidences)\n",
    "supports_df = pd.DataFrame(all_supports)\n",
    "lifts_df = pd.DataFrame(all_lifts)\n",
    "\n",
    "# Display the seasonal mean confidence, support, and lift values\n",
    "print(\"Confidence DataFrame\")\n",
    "print(confidences_df)\n",
    "print(\"Support DataFrame\")\n",
    "print(supports_df)\n",
    "print(\"Lift DataFrame\")\n",
    "print(lifts_df)\n",
    "\n",
    "# Function to perform Kolmogorov-Smirnov test\n",
    "def ks_test(data, metric, season):\n",
    "    stat, p_value = stats.kstest(data, 'norm', args=(data.mean(), data.std()))\n",
    "    print(f\"Kolmogorov-Smirnov test for {metric} in {season}:\")\n",
    "    print(f\"  Test statistic: {stat:.6f}\")\n",
    "    print(f\"  p-value: {p_value:.6f}\")\n",
    "    if p_value > 0.05:\n",
    "        print(f\"  {metric} in {season} data is normally distributed.\\n\")\n",
    "    else:\n",
    "        print(f\"  {metric} in {season} data is not normally distributed.\\n\")\n",
    "\n",
    "# Function to check normality for each metric and season\n",
    "def check_normality(df, metric):\n",
    "    for season in ['spring', 'summer', 'autumn', 'winter']:\n",
    "        season_data = df[df['season'] == season][metric]\n",
    "        if len(season_data) > 0:\n",
    "            ks_test(season_data, metric, season)\n",
    "\n",
    "# Check normality for mean_confidence, mean_support, and mean_lift\n",
    "print(\"Checking normality for mean_confidence\")\n",
    "check_normality(confidences_df, 'mean_confidence')\n",
    "print(\"Checking normality for mean_support\")\n",
    "check_normality(supports_df, 'mean_support')\n",
    "print(\"Checking normality for mean_lift\")\n",
    "check_normality(lifts_df, 'mean_lift')\n",
    "\n",
    "# Perform Kruskal-Wallis test to see if there's a significant difference between the seasonal mean values\n",
    "def perform_kruskal_wallis_test(metric, df):\n",
    "    spring_values = df[df['season'] == 'spring'][metric]\n",
    "    summer_values = df[df['season'] == 'summer'][metric]\n",
    "    autumn_values = df[df['season'] == 'autumn'][metric]\n",
    "    winter_values = df[df['season'] == 'winter'][metric]\n",
    "    \n",
    "    kruskal_test = stats.kruskal(spring_values, summer_values, autumn_values, winter_values)\n",
    "    \n",
    "    print(f\"Kruskal-Wallis test statistic for {metric}: {kruskal_test.statistic:.6f}\")\n",
    "    print(f\"Kruskal-Wallis test p-value for {metric}: {kruskal_test.pvalue:.30f}\")\n",
    "    \n",
    "    if kruskal_test.pvalue < 0.05:\n",
    "        print(f\"There are significant differences in mean {metric} values across the seasons.\")\n",
    "    else:\n",
    "        print(f\"There are no significant differences in mean {metric} values across the seasons.\")\n",
    "\n",
    "# Perform tests for confidence, support, and lift\n",
    "perform_kruskal_wallis_test('mean_confidence', confidences_df)\n",
    "perform_kruskal_wallis_test('mean_support', supports_df)\n",
    "perform_kruskal_wallis_test('mean_lift', lifts_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a429e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Dunn's post hoc test for lift values\n",
    "dunn_test = sp.posthoc_dunn(lifts_df, val_col='mean_lift', group_col='season', p_adjust='bonferroni')\n",
    "print(\"Dunn's test results for lift values across seasons:\")\n",
    "print(dunn_test)\n",
    "\n",
    "# Calculate mean ranks\n",
    "mean_ranks = lifts_df.groupby('season')['mean_lift'].mean().sort_values()\n",
    "print(\"Mean ranks for lift values:\")\n",
    "print(mean_ranks)\n",
    "\n",
    "# Annotate directions on the Dunn's test results\n",
    "seasons = ['spring', 'summer', 'autumn', 'winter']\n",
    "direction_matrix = pd.DataFrame(index=seasons, columns=seasons)\n",
    "for i in seasons:\n",
    "    for j in seasons:\n",
    "        if dunn_test.loc[i, j] < 0.05:  # Significant difference\n",
    "            if mean_ranks[i] > mean_ranks[j]:\n",
    "                direction_matrix.loc[i, j] = f\"{i} > {j}\"\n",
    "            else:\n",
    "                direction_matrix.loc[i, j] = f\"{i} < {j}\"\n",
    "        else:\n",
    "            direction_matrix.loc[i, j] = 'No significant difference'\n",
    "\n",
    "print(\"Direction of Differences:\")\n",
    "print(direction_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b9a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Dunn's post hoc test for support values\n",
    "dunn_test_support = sp.posthoc_dunn(supports_df, val_col='mean_support', group_col='season', p_adjust='bonferroni')\n",
    "print(\"Dunn's test results for support values across seasons:\")\n",
    "print(dunn_test_support)\n",
    "\n",
    "# Calculate mean ranks for support values\n",
    "mean_ranks_support = supports_df.groupby('season')['mean_support'].mean().sort_values()\n",
    "print(\"Mean ranks for support values:\")\n",
    "print(mean_ranks_support)\n",
    "\n",
    "# Annotate directions on the Dunn's test results for support values\n",
    "seasons = ['spring', 'summer', 'autumn', 'winter']\n",
    "direction_matrix_support = pd.DataFrame(index=seasons, columns=seasons)\n",
    "for i in seasons:\n",
    "    for j in seasons:\n",
    "        if dunn_test_support.loc[i, j] < 0.05:  # Significant difference\n",
    "            if mean_ranks_support[i] > mean_ranks_support[j]:\n",
    "                direction_matrix_support.loc[i, j] = f\"{i} > {j}\"\n",
    "            else:\n",
    "                direction_matrix_support.loc[i, j] = f\"{i} < {j}\"\n",
    "        else:\n",
    "            direction_matrix_support.loc[i, j] = 'No significant difference'\n",
    "\n",
    "print(\"Direction of Differences for Support Values:\")\n",
    "print(direction_matrix_support)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'trx_date' column is in datetime format\n",
    "sales_data['trx_date'] = pd.to_datetime(sales_data['trx_date'])\n",
    "\n",
    "# Assign seasons\n",
    "def assign_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'summer'\n",
    "    else:\n",
    "        return 'autumn'\n",
    "\n",
    "sales_data['season'] = sales_data['trx_date'].dt.month.apply(assign_season)\n",
    "\n",
    "# Filter data for winter and summer\n",
    "winter_data = sales_data[sales_data['season'] == 'winter']\n",
    "summer_data = sales_data[sales_data['season'] == 'summer']\n",
    "\n",
    "# Calculate average basket size for winter\n",
    "winter_basket_size = winter_data.groupby('trx_id')['product_code'].count().mean()\n",
    "\n",
    "# Calculate average basket size for summer\n",
    "summer_basket_size = summer_data.groupby('trx_id')['product_code'].count().mean()\n",
    "\n",
    "print(f\"Average basket size in winter: {winter_basket_size}\")\n",
    "print(f\"Average basket size in summer: {summer_basket_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1935c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming sales_data is already loaded as a DataFrame\n",
    "\n",
    "# Ensure the 'trx_date' column is in datetime format\n",
    "sales_data['trx_date'] = pd.to_datetime(sales_data['trx_date'])\n",
    "\n",
    "# Assign seasons\n",
    "def assign_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'summer'\n",
    "    else:\n",
    "        return 'autumn'\n",
    "\n",
    "sales_data['season'] = sales_data['trx_date'].dt.month.apply(assign_season)\n",
    "\n",
    "# Group by season and count the unique product codes\n",
    "products_per_season = sales_data.groupby('season')['product_code'].nunique()\n",
    "\n",
    "print(\"Number of different products sold per season:\")\n",
    "print(products_per_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Load your sales data into a DataFrame (assuming it's already done)\n",
    "# sales_data = pd.read_csv('your_sales_data.csv')\n",
    "\n",
    "# Filter transactions that include Product ID_44\n",
    "product_id = 'ID_44'\n",
    "transactions_with_product = sales_data[sales_data['product_code'] == product_id]\n",
    "\n",
    "# Get the transaction IDs for transactions that include Product ID_44\n",
    "transaction_ids = transactions_with_product['trx_id'].unique()\n",
    "\n",
    "# Filter the sales data to include only these transactions\n",
    "filtered_sales_data = sales_data[sales_data['trx_id'].isin(transaction_ids)]\n",
    "\n",
    "# Count the frequency of other products bought with Product ID_44\n",
    "product_counts = Counter(filtered_sales_data['product_code'])\n",
    "\n",
    "# Count the number of transactions where Product ID_44 was bought alone\n",
    "transactions_with_product_alone = transactions_with_product['trx_id'].isin(\n",
    "    filtered_sales_data.groupby('trx_id').filter(lambda x: len(x) == 1)['trx_id']\n",
    ").sum()\n",
    "\n",
    "# Remove Product ID_44 from the count for other products\n",
    "del product_counts[product_id]\n",
    "\n",
    "# Convert the counter to a DataFrame for easy viewing\n",
    "product_counts_df = pd.DataFrame(product_counts.items(), columns=['Product_Code', 'Count'])\n",
    "product_counts_df = product_counts_df.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Display the top 10 products bought with Product ID_44\n",
    "print(\"Product ID_44 bought alone:\", transactions_with_product_alone)\n",
    "print(product_counts_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a246a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure the 'trx_date' column is in datetime format\n",
    "sales_data['trx_date'] = pd.to_datetime(sales_data['trx_date'])\n",
    "\n",
    "# Filter transactions from the year 2020\n",
    "sales_data_2020 = sales_data[sales_data['trx_date'].dt.year == 2020]\n",
    "\n",
    "# Filter transactions that include Product ID_44\n",
    "product_id = 'ID_60'\n",
    "transactions_with_product = sales_data_2020[sales_data_2020['product_code'] == product_id]\n",
    "\n",
    "# Get the transaction IDs for transactions that include Product ID_44\n",
    "transaction_ids = transactions_with_product['trx_id'].unique()\n",
    "\n",
    "# Filter the sales data to include only these transactions\n",
    "filtered_sales_data = sales_data_2020[sales_data_2020['trx_id'].isin(transaction_ids)]\n",
    "\n",
    "# Count the frequency of other products bought with Product ID_44\n",
    "product_counts = Counter(filtered_sales_data['product_code'])\n",
    "\n",
    "# Count the number of transactions where Product ID_44 was bought alone\n",
    "transactions_with_product_alone = transactions_with_product['trx_id'].isin(\n",
    "    filtered_sales_data.groupby('trx_id').filter(lambda x: len(x) == 1)['trx_id']\n",
    ").sum()\n",
    "\n",
    "# Remove Product ID_44 from the count for other products\n",
    "del product_counts[product_id]\n",
    "\n",
    "# Convert the counter to a DataFrame for easy viewing\n",
    "product_counts_df = pd.DataFrame(product_counts.items(), columns=['Product_Code', 'Count'])\n",
    "product_counts_df = product_counts_df.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Display the top 10 products bought with Product ID_44\n",
    "print(\"Product ID_44 bought alone:\", transactions_with_product_alone)\n",
    "print(product_counts_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bcc00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure the 'trx_date' column is in datetime format\n",
    "sales_data['trx_date'] = pd.to_datetime(sales_data['trx_date'])\n",
    "\n",
    "# Filter transactions from the year 2020\n",
    "sales_data_2020 = sales_data[sales_data['trx_date'].dt.year == 2020]\n",
    "\n",
    "# Filter transactions that include Product ID_44 and Product ID_60\n",
    "product_id_44 = 'ID_44'\n",
    "product_id_60 = 'ID_60'\n",
    "\n",
    "transactions_with_product_44 = sales_data_2020[sales_data_2020['product_code'] == product_id_44]\n",
    "transactions_with_product_60 = sales_data_2020[sales_data_2020['product_code'] == product_id_60]\n",
    "\n",
    "# Get the transaction IDs for transactions that include Product ID_44 and Product ID_60\n",
    "transaction_ids_44 = set(transactions_with_product_44['trx_id'].unique())\n",
    "transaction_ids_60 = set(transactions_with_product_60['trx_id'].unique())\n",
    "\n",
    "# Find transactions that include both Product ID_44 and Product ID_60\n",
    "common_transaction_ids = transaction_ids_44.intersection(transaction_ids_60)\n",
    "\n",
    "# Count the occurrences\n",
    "total_occurrences_44 = len(transactions_with_product_44)\n",
    "total_occurrences_60 = len(transactions_with_product_60)\n",
    "together_count = len(common_transaction_ids)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total occurrences of Product ID_44 in 2020: {total_occurrences_44}\")\n",
    "print(f\"Total occurrences of Product ID_60 in 2020: {total_occurrences_60}\")\n",
    "print(f\"Number of times Product ID_44 and Product ID_60 were bought together in 2020: {together_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bcb9d2",
   "metadata": {},
   "source": [
    "# Hypotheses 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed17ac3f",
   "metadata": {},
   "source": [
    "## Shops with similar customer purchase patterns (clustered based on association rules) show similar sales growth rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e909f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `sales_data` is the DataFrame with the provided structure\n",
    "\n",
    "# Step 1: Extract Total Annual Sales per Shop\n",
    "annual_sales = sales_data.groupby(['shopID', 'year_integer'])['total_value'].sum().reset_index()\n",
    "\n",
    "# Step 2: Calculate Sales Growth Rates for Each Shop\n",
    "annual_sales['sales_growth'] = annual_sales.groupby('shopID')['total_value'].pct_change()\n",
    "\n",
    "# Remove the first year for each shop as it will have NaN sales growth\n",
    "annual_sales = annual_sales.dropna(subset=['sales_growth'])\n",
    "\n",
    "# Define the clusters\n",
    "cluster_1 = [1, 3, 5]\n",
    "cluster_2 = [6, 7, 8, 9]\n",
    "cluster_3 = [4]\n",
    "cluster_4 = [11, 12, 13]\n",
    "\n",
    "# Step 3: Separate the growth rates by cluster\n",
    "cluster_1_growth = annual_sales[annual_sales['shopID'].isin(cluster_1)]['sales_growth']\n",
    "cluster_2_growth = annual_sales[annual_sales['shopID'].isin(cluster_2)]['sales_growth']\n",
    "cluster_3_growth = annual_sales[annual_sales['shopID'].isin(cluster_3)]['sales_growth']\n",
    "cluster_4_growth = annual_sales[annual_sales['shopID'].isin(cluster_4)]['sales_growth']\n",
    "\n",
    "# Step 4: Perform ANOVA test to compare the sales growth rates between clusters\n",
    "anova_result = stats.f_oneway(cluster_1_growth, cluster_2_growth, cluster_3_growth, cluster_4_growth)\n",
    "\n",
    "print(f\"Cluster 1 sales growth rates: {cluster_1_growth.values}\")\n",
    "print(f\"Cluster 2 sales growth rates: {cluster_2_growth.values}\")\n",
    "print(f\"Cluster 3 sales growth rates: {cluster_3_growth.values}\")\n",
    "print(f\"Cluster 4 sales growth rates: {cluster_4_growth.values}\")\n",
    "print(f\"ANOVA test statistic: {anova_result.statistic:.6f}\")\n",
    "print(f\"ANOVA test p-value: {anova_result.pvalue:.6f}\")\n",
    "\n",
    "if anova_result.pvalue < 0.05:\n",
    "    print(\"There are significant differences in sales growth rates between the clusters.\")\n",
    "else:\n",
    "    print(\"There are no significant differences in sales growth rates between the clusters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7833048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the trx_date column is in datetime format\n",
    "sales_data['trx_date'] = pd.to_datetime(sales_data['trx_date'])\n",
    "\n",
    "# Extract the year and quarter from the trx_date\n",
    "sales_data['year_quarter'] = sales_data['trx_date'].dt.to_period('Q')\n",
    "\n",
    "# Step 1: Extract Total Quarterly Sales per Shop\n",
    "quarterly_sales = sales_data.groupby(['shopID', 'year_quarter'])['total_value'].sum().reset_index()\n",
    "\n",
    "# Step 2: Calculate Quarterly Growth Rates for Each Shop\n",
    "quarterly_sales['quarterly_growth'] = quarterly_sales.groupby('shopID')['total_value'].pct_change()\n",
    "\n",
    "# Remove the first quarter for each shop as it will have NaN sales growth\n",
    "quarterly_sales = quarterly_sales.dropna(subset=['quarterly_growth'])\n",
    "\n",
    "# Define the clusters\n",
    "cluster_1 = [1, 3, 5]\n",
    "cluster_2 = [6, 7, 8, 9]\n",
    "cluster_4 = [11, 12, 13]\n",
    "\n",
    "# Function to check normality within a cluster\n",
    "def check_normality_within_cluster(cluster, cluster_name):\n",
    "    cluster_growth = quarterly_sales[quarterly_sales['shopID'].isin(cluster)]\n",
    "    normality_results = {}\n",
    "    for shop in cluster:\n",
    "        shop_growth = cluster_growth[cluster_growth['shopID'] == shop]['quarterly_growth']\n",
    "        if len(shop_growth) >= 3:\n",
    "            result = stats.shapiro(shop_growth)\n",
    "            normality_results[shop] = result\n",
    "            print(f\"Shop {shop}: W-statistic={result.statistic:.6f}, p-value={result.pvalue:.6f}\")\n",
    "            if result.pvalue < 0.05:\n",
    "                print(f\"Shop {shop}'s growth rates are not normally distributed.\")\n",
    "            else:\n",
    "                print(f\"Shop {shop}'s growth rates are normally distributed.\")\n",
    "        else:\n",
    "            print(f\"Shop {shop} does not have enough data points for normality test.\")\n",
    "            normality_results[shop] = None\n",
    "    print()\n",
    "    \n",
    "    return normality_results\n",
    "\n",
    "# Function to perform ANOVA test within a cluster\n",
    "def perform_anova_within_cluster(cluster, cluster_name):\n",
    "    cluster_growth = quarterly_sales[quarterly_sales['shopID'].isin(cluster)]\n",
    "    anova_result = stats.f_oneway(\n",
    "        *[cluster_growth[cluster_growth['shopID'] == shop]['quarterly_growth'] for shop in cluster]\n",
    "    )\n",
    "    \n",
    "    print(f\"ANOVA test results for {cluster_name}:\")\n",
    "    print(f\"ANOVA test statistic: {anova_result.statistic:.6f}\")\n",
    "    print(f\"ANOVA test p-value: {anova_result.pvalue:.6f}\")\n",
    "    \n",
    "    if anova_result.pvalue < 0.05:\n",
    "        print(f\"There are significant differences in quarterly growth rates within {cluster_name}.\")\n",
    "    else:\n",
    "        print(f\"There are no significant differences in quarterly growth rates within {cluster_name}.\")\n",
    "    print()\n",
    "\n",
    "# Check normality and perform ANOVA test within each cluster\n",
    "normality_results_cluster_1 = check_normality_within_cluster(cluster_1, 'Cluster 1')\n",
    "normality_results_cluster_2 = check_normality_within_cluster(cluster_2, 'Cluster 2')\n",
    "normality_results_cluster_4 = check_normality_within_cluster(cluster_4, 'Cluster 4')\n",
    "\n",
    "perform_anova_within_cluster(cluster_1, 'Cluster 1')\n",
    "perform_anova_within_cluster(cluster_2, 'Cluster 2')\n",
    "perform_anova_within_cluster(cluster_4, 'Cluster 4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a42fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the trx_date column is in datetime format\n",
    "sales_data['trx_date'] = pd.to_datetime(sales_data['trx_date'])\n",
    "\n",
    "# Extract the year and quarter from the trx_date\n",
    "sales_data['year_quarter'] = sales_data['trx_date'].dt.to_period('Q')\n",
    "\n",
    "# Step 1: Extract Total Quarterly Sales per Shop\n",
    "quarterly_sales = sales_data.groupby(['shopID', 'year_quarter'])['total_value'].sum().reset_index()\n",
    "\n",
    "# Step 2: Calculate Quarterly Growth Rates for Each Shop\n",
    "quarterly_sales['quarterly_growth'] = quarterly_sales.groupby('shopID')['total_value'].pct_change()\n",
    "\n",
    "# Remove the first quarter for each shop as it will have NaN sales growth\n",
    "quarterly_sales = quarterly_sales.dropna(subset=['quarterly_growth'])\n",
    "\n",
    "# Define the clusters\n",
    "cluster_1 = [1, 3, 5]\n",
    "cluster_2 = [6, 7, 8, 9]\n",
    "cluster_4 = [11, 12, 13]\n",
    "\n",
    "# Function to perform Kruskal-Wallis test within a cluster\n",
    "def perform_kruskal_wallis_within_cluster(cluster, cluster_name):\n",
    "    cluster_growth = quarterly_sales[quarterly_sales['shopID'].isin(cluster)]\n",
    "    kruskal_result = stats.kruskal(\n",
    "        *[cluster_growth[cluster_growth['shopID'] == shop]['quarterly_growth'] for shop in cluster]\n",
    "    )\n",
    "    \n",
    "    print(f\"Kruskal-Wallis test results for {cluster_name}:\")\n",
    "    print(f\"Kruskal-Wallis test statistic: {kruskal_result.statistic:.6f}\")\n",
    "    print(f\"Kruskal-Wallis test p-value: {kruskal_result.pvalue:.6f}\")\n",
    "    \n",
    "    if kruskal_result.pvalue < 0.05:\n",
    "        print(f\"There are significant differences in quarterly growth rates within {cluster_name}.\")\n",
    "    else:\n",
    "        print(f\"There are no significant differences in quarterly growth rates within {cluster_name}.\")\n",
    "    print()\n",
    "\n",
    "# Perform Kruskal-Wallis test within each cluster\n",
    "perform_kruskal_wallis_within_cluster(cluster_1, 'Cluster 1')\n",
    "perform_kruskal_wallis_within_cluster(cluster_2, 'Cluster 2')\n",
    "perform_kruskal_wallis_within_cluster(cluster_4, 'Cluster 4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee028783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the trx_date column is in datetime format\n",
    "sales_data['trx_date'] = pd.to_datetime(sales_data['trx_date'])\n",
    "\n",
    "# Extract the year and quarter from the trx_date\n",
    "sales_data['year_quarter'] = sales_data['trx_date'].dt.to_period('Q')\n",
    "\n",
    "# Step 1: Extract Total Quarterly Sales per Shop\n",
    "quarterly_sales = sales_data.groupby(['shopID', 'year_quarter'])['total_value'].sum().reset_index()\n",
    "\n",
    "# Step 2: Calculate Quarterly Growth Rates for Each Shop\n",
    "quarterly_sales['quarterly_growth'] = quarterly_sales.groupby('shopID')['total_value'].pct_change()\n",
    "\n",
    "# Remove the first quarter for each shop as it will have NaN sales growth\n",
    "quarterly_sales = quarterly_sales.dropna(subset=['quarterly_growth'])\n",
    "\n",
    "# Define the clusters\n",
    "cluster_1 = [1, 3, 5]\n",
    "cluster_2 = [6, 7, 8, 9]\n",
    "cluster_4 = [11, 12, 13]\n",
    "\n",
    "# Function to calculate Coefficient of Variation (CV) for a cluster\n",
    "def calculate_cv(cluster, cluster_name):\n",
    "    cluster_growth = quarterly_sales[quarterly_sales['shopID'].isin(cluster)]\n",
    "    cv = cluster_growth.groupby('shopID')['quarterly_growth'].std() / cluster_growth.groupby('shopID')['quarterly_growth'].mean()\n",
    "    print(f\"Coefficient of Variation (CV) for {cluster_name}:\")\n",
    "    print(cv)\n",
    "    print(f\"Mean CV for {cluster_name}: {cv.mean()}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# Perform CV within each cluster\n",
    "calculate_cv(cluster_1, 'Cluster 1')\n",
    "calculate_cv(cluster_2, 'Cluster 2')\n",
    "calculate_cv(cluster_4, 'Cluster 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53dfea6",
   "metadata": {},
   "source": [
    "# Recommendation Algorithm Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d898c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract product codes from frozenset data types in antecedents and consequents\n",
    "def extract_product_codes(antecedents, consequents):\n",
    "    return list(antecedents.union(consequents))\n",
    "\n",
    "# Function to create a rule representation from antecedents and consequents\n",
    "def create_rule_string(antecedents, consequents):\n",
    "    return f\"{antecedents} -> {consequents}\"\n",
    "\n",
    "# Function to normalize a list of metrics\n",
    "def normalize_metric(metric):\n",
    "    min_val = min(metric)\n",
    "    max_val = max(metric)\n",
    "    return [(m - min_val) / (max_val - min_val) for m in metric]\n",
    "\n",
    "# Function to recommend products based on given products\n",
    "def recommend_products(meta_dfs_redundant, input_products, total_metric_threshold=2, lift_weight=2):\n",
    "    recommendations = defaultdict(list)\n",
    "    \n",
    "    for shop, years_data in meta_dfs_redundant.items():\n",
    "        for year, df in years_data.items():\n",
    "            for _, row in df.iterrows():\n",
    "                antecedents = row['antecedents']\n",
    "                if antecedents.issubset(input_products):\n",
    "                    recommendations[row['consequents']].append((row['lift'], row['confidence'], row['support'], row['conviction']))\n",
    "\n",
    "    if not recommendations:\n",
    "        print(\"No recommendations found.\")\n",
    "        return []\n",
    "\n",
    "    # Flatten the recommendations\n",
    "    recommendations_list = []\n",
    "    for consequent, metrics in recommendations.items():\n",
    "        avg_metrics = tuple(map(lambda x: sum(x) / len(x), zip(*metrics)))\n",
    "        recommendations_list.append((consequent, avg_metrics[0], avg_metrics[1], avg_metrics[2], avg_metrics[3]))\n",
    "\n",
    "    # Normalize metrics\n",
    "    lifts = [rec[1] for rec in recommendations_list]\n",
    "    confidences = [rec[2] for rec in recommendations_list]\n",
    "    supports = [rec[3] for rec in recommendations_list]\n",
    "    convictions = [rec[4] for rec in recommendations_list]\n",
    "\n",
    "    normalized_lifts = normalize_metric(lifts)\n",
    "    normalized_confidences = normalize_metric(confidences)\n",
    "    normalized_supports = normalize_metric(supports)\n",
    "    normalized_convictions = normalize_metric(convictions)\n",
    "\n",
    "    # Calculate the total metric and update recommendations_list\n",
    "    total_metrics = [\n",
    "        normalized_lifts[i] * lift_weight + normalized_confidences[i] + normalized_supports[i] + normalized_convictions[i]\n",
    "        for i in range(len(recommendations_list))\n",
    "    ]\n",
    "    for i in range(len(recommendations_list)):\n",
    "        recommendations_list[i] = recommendations_list[i] + (total_metrics[i],)\n",
    "\n",
    "    # Sort by the total metric\n",
    "    recommendations_list.sort(key=lambda x: x[5], reverse=True)\n",
    "\n",
    "    if not recommendations_list:\n",
    "        print(\"All recommendations were excluded.\")\n",
    "        return []\n",
    "\n",
    "    # Exclude items that are part of the input\n",
    "    recommendations_list = [rec for rec in recommendations_list if not any(item in input_products for item in rec[0])]\n",
    "\n",
    "    if not recommendations_list:\n",
    "        print(\"All recommendations were excluded after filtering input products.\")\n",
    "        return []\n",
    "\n",
    "    # Filter out recommendations with total_metric less than 1\n",
    "    recommendations_list = [rec for rec in recommendations_list if rec[5] >= 1]\n",
    "\n",
    "    if not recommendations_list:\n",
    "        print(\"All recommendations were excluded due to low total_metric.\")\n",
    "        return []\n",
    "\n",
    "    if len(recommendations_list) < 5:\n",
    "        return [(rec[0], rec[5]) for rec in recommendations_list]\n",
    "\n",
    "    # Check if the step between the first and subsequent recommendations is not very high using total metric values\n",
    "    results = [(recommendations_list[0][0], recommendations_list[0][5])]\n",
    "    for i in range(1, min(5, len(recommendations_list))):\n",
    "        current_total_metric = recommendations_list[i][5]\n",
    "        previous_total_metric = recommendations_list[i - 1][5]\n",
    "        if (previous_total_metric - current_total_metric) / previous_total_metric > total_metric_threshold:\n",
    "            break\n",
    "        results.append((recommendations_list[i][0], current_total_metric))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d631023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by trx_id and count the number of products in each transaction\n",
    "products_per_transaction = sales_data.groupby('trx_id').size()\n",
    "\n",
    "# Calculate the average number of products per transaction\n",
    "average_products_per_transaction = products_per_transaction.mean()\n",
    "\n",
    "print(f\"Average number of products per transaction: {average_products_per_transaction:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fe82ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for the year 2023\n",
    "sales_data_2023 = sales_data[sales_data['year_integer'] == 2023]\n",
    "\n",
    "# Group the data by trx_id and count the number of products in each transaction for the year 2023\n",
    "products_per_transaction_2023 = sales_data_2023.groupby('trx_id').size()\n",
    "\n",
    "# Calculate the average number of products per transaction for the year 2023\n",
    "average_products_per_transaction_2023 = products_per_transaction_2023.mean()\n",
    "\n",
    "print(f\"Average number of products per transaction in 2023: {average_products_per_transaction_2023:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf846df",
   "metadata": {},
   "source": [
    "# Recommendation Algorithm Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c266d4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input: single product\n",
    "input_products = {'ID_44'}\n",
    "\n",
    "# Call the function\n",
    "recommendations = recommend_products(meta_dfs_redundant, input_products)\n",
    "\n",
    "# Print recommendations with paragraph breaks\n",
    "print(\"\\n\\n\".join(f\"Recommended product: {', '.join(product)}, Total Metric: {total_metric:.2f}\" for product, total_metric in recommendations))\n",
    "\n",
    "# Extract product IDs and their respective total metric values for plotting\n",
    "products = [', '.join(list(rec[0])) for rec in recommendations][:5]\n",
    "total_metrics = [rec[1] for rec in recommendations][:5]\n",
    "\n",
    "# Plot the horizontal bar graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.barh(products, total_metrics, color=plt.cm.Reds(np.array(total_metrics) / max(total_metrics)))\n",
    "\n",
    "# Annotate the bars with product names and total_metric values\n",
    "for bar, product, metric in zip(bars, products, total_metrics):\n",
    "    plt.text(bar.get_width()/2, bar.get_y() + bar.get_height()/2,\n",
    "             f\"{product}, Strength: {metric:.2f}\", ha='center', va='center', fontsize=10, color='white')\n",
    "\n",
    "# Remove x-axis and y-axis labels, ticks, and the coordinate system\n",
    "plt.gca().xaxis.set_visible(False)\n",
    "plt.gca().yaxis.set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "\n",
    "# Reverse the order of the products on the y-axis\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Save the plot as an SVG file\n",
    "plt.savefig(\"recommended_products1.svg\", format='svg')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c160734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input: single product\n",
    "input_products = {'ID_8', 'ID_16', 'ID_9'}\n",
    "# Call the function\n",
    "recommendations = recommend_products(meta_dfs_redundant, input_products)\n",
    "\n",
    "# Print recommendations with paragraph breaks\n",
    "print(\"\\n\\n\".join(f\"Recommended product: {', '.join(product)}, Total Metric: {total_metric:.2f}\" for product, total_metric in recommendations))\n",
    "\n",
    "# Extract product IDs and their respective total metric values for plotting\n",
    "products = [', '.join(list(rec[0])) for rec in recommendations][:5]\n",
    "total_metrics = [rec[1] for rec in recommendations][:5]\n",
    "\n",
    "# Plot the horizontal bar graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.barh(products, total_metrics, color=plt.cm.Reds(np.array(total_metrics) / max(total_metrics)))\n",
    "\n",
    "# Annotate the bars with product names and total_metric values\n",
    "for bar, product, metric in zip(bars, products, total_metrics):\n",
    "    plt.text(bar.get_width()/2, bar.get_y() + bar.get_height()/2,\n",
    "             f\"{product}, Strength: {metric:.2f}\", ha='center', va='center', fontsize=10, color='white')\n",
    "\n",
    "# Remove x-axis and y-axis labels, ticks, and the coordinate system\n",
    "plt.gca().xaxis.set_visible(False)\n",
    "plt.gca().yaxis.set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "\n",
    "# Reverse the order of the products on the y-axis\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Save the plot as an SVG file\n",
    "plt.savefig(\"recommended_products2.svg\", format='svg')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9983d712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input: single product\n",
    "input_products = {'ID_27','ID_400', 'ID_95'}\n",
    "\n",
    "# Call the function\n",
    "recommendations = recommend_products(meta_dfs_redundant, input_products)\n",
    "\n",
    "# Print recommendations with paragraph breaks\n",
    "print(\"\\n\\n\".join(f\"Recommended product: {', '.join(product)}, Total Metric: {total_metric:.2f}\" for product, total_metric in recommendations))\n",
    "\n",
    "# Extract product IDs and their respective total metric values for plotting\n",
    "products = [', '.join(list(rec[0])) for rec in recommendations][:5]\n",
    "total_metrics = [rec[1] for rec in recommendations][:5]\n",
    "\n",
    "# Plot the horizontal bar graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.barh(products, total_metrics, color=plt.cm.Reds(np.array(total_metrics) / max(total_metrics)))\n",
    "\n",
    "# Annotate the bars with product names and total_metric values\n",
    "for bar, product, metric in zip(bars, products, total_metrics):\n",
    "    plt.text(bar.get_width()/2, bar.get_y() + bar.get_height()/2,\n",
    "             f\"{product}, Strength: {metric:.2f}\", ha='center', va='center', fontsize=10, color='white')\n",
    "\n",
    "# Remove x-axis and y-axis labels, ticks, and the coordinate system\n",
    "plt.gca().xaxis.set_visible(False)\n",
    "plt.gca().yaxis.set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "\n",
    "# Reverse the order of the products on the y-axis\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Save the plot as an SVG file\n",
    "plt.savefig(\"recommended_products3.svg\", format='svg')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e48a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input: single product\n",
    "input_products = {'ID_64', 'ID_32'}\n",
    "\n",
    "# Call the function\n",
    "recommendations = recommend_products(meta_dfs_redundant, input_products)\n",
    "\n",
    "# Print recommendations with paragraph breaks\n",
    "print(\"\\n\\n\".join(f\"Recommended product: {', '.join(product)}, Total Metric: {total_metric:.2f}\" for product, total_metric in recommendations))\n",
    "\n",
    "# Extract product IDs and their respective total metric values for plotting\n",
    "products = [', '.join(list(rec[0])) for rec in recommendations][:5]\n",
    "total_metrics = [rec[1] for rec in recommendations][:5]\n",
    "\n",
    "# Plot the horizontal bar graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.barh(products, total_metrics, color=plt.cm.Reds(np.array(total_metrics) / max(total_metrics)))\n",
    "\n",
    "# Annotate the bars with product names and total_metric values\n",
    "for bar, product, metric in zip(bars, products, total_metrics):\n",
    "    plt.text(bar.get_width()/2, bar.get_y() + bar.get_height()/2,\n",
    "             f\"{product}, Strength: {metric:.2f}\", ha='center', va='center', fontsize=10, color='white')\n",
    "\n",
    "# Remove x-axis and y-axis labels, ticks, and the coordinate system\n",
    "plt.gca().xaxis.set_visible(False)\n",
    "plt.gca().yaxis.set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "\n",
    "# Reverse the order of the products on the y-axis\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Save the plot as an SVG file\n",
    "plt.savefig(\"recommended_products3.svg\", format='svg')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
